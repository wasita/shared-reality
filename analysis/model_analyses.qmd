---
title: "Computational Model Analyses"
author: "Reproducibility Code"
format:
  html:
    code-fold: true
    toc: true
    toc-depth: 3
jupyter: python3
execute:
  warning: false
  message: false
---

This document reproduces the main computational model analyses (Figures 4-5).

- **Section 1**: Setup and data loading
- **Section 2**: Parameter fitting (grid search)
- **Section 3**: Figure 4 (Bayesian model predictions)
- **Section 4**: Figure 5 (LLM analysis)
- **Section 5**: Summary statistics

See `supplement.qmd` for additional analyses (k-sweep, factor structure, scrambled control, self-structure diagnostic).

## 1. Setup

```{python}
#| label: setup
import sys
from pathlib import Path
import json
import numpy as np
import pandas as pd
import matplotlib.pyplot as plt

# Find project root
base_dir = Path.cwd().resolve()
while base_dir.name and not (base_dir / 'data').exists():
    base_dir = base_dir.parent
sys.path.insert(0, str(base_dir))

# Import model and utilities
from models.model import (
    CommonalityModel,
    load_factor_loadings,
    load_evaluation_data,
    prepare_evaluation_data,
    fast_evaluate,
    DOMAIN_RANGES,
)
from analysis.utils import (
    get_rates,
    compute_gradient,
    bootstrap_gradient,
    bootstrap_rates,
    set_plot_style,
    add_significance_bracket,
    COLORS,
)

set_plot_style()
print(f'Project root: {base_dir}')
```

```{python}
#| label: load-data
# Load behavioral data
data = load_evaluation_data()
print(f'Loaded {len(data)} observations from {data["pid"].nunique()} participants')

# Compute human rates from data
HUMAN_RATES = get_rates(data, 'participant_binary_prediction')
print('\nHuman rates:')
for (qt, mt), rate in sorted(HUMAN_RATES.items()):
    print(f'  {qt}, {mt}: {rate:.3f}')
```

## 2. Parameter Fitting

We fit parameters on the no-chat condition where observations are exact (participants see explicit responses). Note: σ_obs is fixed at 0 for this condition.

```{python}
#| label: fit-params
from models.model import fit_parameters

# Prepare no-chat data (information-matched to Bayesian model)
nochat_data = data[data['experiment'] == 'no-chat'].copy()
nochat_eval = prepare_evaluation_data(nochat_data)
NOCHAT_HUMAN = get_rates(nochat_data, 'participant_binary_prediction')

# Load cached params or refit
params_path = base_dir / 'models' / 'fitted_params.json'

if params_path.exists():
    with open(params_path) as f:
        fitted_cache = json.load(f)
    FITTED_PARAMS = fitted_cache['k5_params']
    UNIFIED_PARAMS = fitted_cache['unified_params']
    print("Loaded fitted parameters:")
    print(f"  k=5 optimal: σ_prior={FITTED_PARAMS['sigma_prior']:.3f}, "
          f"τ={FITTED_PARAMS['match_threshold']:.3f}, ε={FITTED_PARAMS['epsilon']:.3f}")
else:
    print("Fitting parameters (this may take a moment)...")
    FITTED_PARAMS, _ = fit_parameters(
        k_values=[5], eval_data=nochat_eval, human_rates=NOCHAT_HUMAN
    )
    print(f"Fitted: σ_prior={FITTED_PARAMS['sigma_prior']:.3f}, "
          f"τ={FITTED_PARAMS['match_threshold']:.3f}, ε={FITTED_PARAMS['epsilon']:.3f}")
```

## 3. Figure 4: Bayesian Model Captures Structured Generalization

The Bayesian model receives only a single Likert response, so we evaluate on the no-chat condition where human participants also receive only this information.

### Panel A: Model Predictions

```{python}
#| label: fig4-setup
# Evaluate k=5 model
model_k5 = CommonalityModel(k=5, lambda_mix=0.0, **FITTED_PARAMS)
nochat_preds = fast_evaluate(model_k5, nochat_eval)
nochat_preds['domain'] = nochat_preds['question_domain'].str.lower()

# Compute rates and bootstrap CIs
model_rates = get_rates(nochat_preds, 'pred_prob')
rate_cis = bootstrap_rates(nochat_preds, 'pred_prob')
```

```{python}
#| label: fig4a
#| fig-width: 4
#| fig-height: 4

fig, ax = plt.subplots(figsize=(4, 4))

qt_keys = ['observed', 'same_domain', 'different_domain']
qt_labels = ['Observed', 'Same\nDomain', 'Different\nDomain']
x = np.arange(len(qt_keys))
width = 0.35

for i, mt in enumerate(['low', 'high']):
    offset = -width/2 if mt == 'low' else width/2
    rates = [model_rates[(qt, mt)] for qt in qt_keys]
    yerr = [[model_rates[(qt, mt)] - rate_cis[(qt, mt)][0] for qt in qt_keys],
            [rate_cis[(qt, mt)][1] - model_rates[(qt, mt)] for qt in qt_keys]]
    ax.bar(x + offset, rates, width, color=COLORS[mt], label=f'{mt.capitalize()} match',
           yerr=yerr, capsize=0, error_kw={'linewidth': 1.5})

ax.set_xticks(x)
ax.set_xticklabels(qt_labels)
ax.set_ylabel('Model P(shared)', fontweight='bold')
ax.set_ylim(0, 1.0)
ax.legend(frameon=False, loc='upper right')
ax.set_title('A', fontweight='bold', loc='left', fontsize=14)

plt.tight_layout()
plt.show()
```

### Panel B: Domain-Level Fit

```{python}
#| label: fig4b
#| fig-width: 4
#| fig-height: 4

fig, ax = plt.subplots(figsize=(4, 4))

# Compute domain-level effects
domain_data = []
for domain in DOMAIN_RANGES.keys():
    for qt in ['same_domain', 'different_domain']:
        subset = nochat_preds[(nochat_preds['domain'] == domain) &
                              (nochat_preds['question_type'] == qt)]
        if len(subset) == 0:
            continue
        high = subset[subset['match_type'] == 'high']
        low = subset[subset['match_type'] == 'low']
        if len(high) == 0 or len(low) == 0:
            continue
        domain_data.append({
            'domain': domain,
            'question_type': qt,
            'model_effect': high['pred_prob'].mean() - low['pred_prob'].mean(),
            'human_effect': high['actual'].mean() - low['actual'].mean()
        })

domain_df = pd.DataFrame(domain_data)

# Plot scatter
for qt in ['same_domain', 'different_domain']:
    subset = domain_df[domain_df['question_type'] == qt]
    color = COLORS['bayesian'] if qt == 'same_domain' else COLORS['scrambled']
    marker = 'o' if qt == 'same_domain' else 's'
    label = 'Same domain' if qt == 'same_domain' else 'Different domain'
    ax.scatter(subset['model_effect'], subset['human_effect'],
               c=color, marker=marker, s=80, edgecolors='white', linewidth=0.5, label=label)

# Correlation and identity line
r = np.corrcoef(domain_df['model_effect'], domain_df['human_effect'])[0, 1]
ax.text(0.05, 0.95, f'r = {r:.2f}', transform=ax.transAxes, fontsize=12, fontweight='bold')

lims = [-0.02, max(ax.get_xlim()[1], ax.get_ylim()[1]) + 0.02]
ax.plot(lims, lims, '--', color='gray', alpha=0.3, linewidth=1, zorder=0)
ax.set_xlim(lims)
ax.set_ylim(lims)

ax.set_xlabel('Model effect', fontweight='bold')
ax.set_ylabel('Human effect', fontweight='bold')
ax.legend(frameon=False, loc='lower right')
ax.set_title('B', fontweight='bold', loc='left', fontsize=14)

plt.tight_layout()
plt.show()
```

### Panel C: Model Comparison

```{python}
#| label: fig4c
#| fig-width: 5
#| fig-height: 4

fig, ax = plt.subplots(figsize=(5, 4))

# Compute gradients
human_gradient = compute_gradient(nochat_preds, 'actual')
bayes_gradient = compute_gradient(nochat_preds, 'pred_prob')
human_boots = bootstrap_gradient(nochat_preds, 'actual')
bayes_boots = bootstrap_gradient(nochat_preds, 'pred_prob')

# Similarity Projection model
m_proj = CommonalityModel(k=0, lambda_mix=1.0, **FITTED_PARAMS,
                          base_rate=0.2, projection_weight=0.8)
proj_preds = fast_evaluate(m_proj, nochat_eval)
proj_preds['pid'] = nochat_preds['pid'].values
proj_gradient = compute_gradient(proj_preds, 'pred_prob')
proj_boots = bootstrap_gradient(proj_preds, 'pred_prob')

# Scrambled control
real_loadings = load_factor_loadings(k=5)
np.random.seed(42)
shuffled_grads = []
for seed in range(100):
    np.random.seed(seed)
    perm = np.random.permutation(35)
    m = CommonalityModel(k=5, lambda_mix=0.0, loadings=real_loadings[perm, :], **FITTED_PARAMS)
    preds = fast_evaluate(m, nochat_eval)
    shuffled_grads.append(compute_gradient(preds, 'pred_prob'))
scrambled_gradient = np.mean(shuffled_grads)
scrambled_ci = [np.percentile(shuffled_grads, 2.5), np.percentile(shuffled_grads, 97.5)]

# Plot
labels = ['Human', 'Bayesian\n(k=5)', 'Similarity\nProjection', 'Scrambled']
gradients = [human_gradient, bayes_gradient, proj_gradient, scrambled_gradient]
colors = [COLORS['human'], COLORS['bayesian'], COLORS['egocentric'], COLORS['scrambled']]
cis = [
    (np.percentile(human_boots, 2.5), np.percentile(human_boots, 97.5)),
    (np.percentile(bayes_boots, 2.5), np.percentile(bayes_boots, 97.5)),
    (np.percentile(proj_boots, 2.5), np.percentile(proj_boots, 97.5)),
    scrambled_ci
]

yerr_lower = [g - ci[0] for g, ci in zip(gradients, cis)]
yerr_upper = [ci[1] - g for g, ci in zip(gradients, cis)]

ax.bar(range(len(gradients)), gradients, color=colors,
       yerr=[yerr_lower, yerr_upper], capsize=0, error_kw={'linewidth': 1.5})
ax.set_xticks(range(len(labels)))
ax.set_xticklabels(labels, fontsize=10)
ax.set_ylabel('Gradient (same − diff)', fontweight='bold')
ax.set_ylim(0, max([g + e for g, e in zip(gradients, yerr_upper)]) * 1.25)

# Significance bracket
bracket_y = max(bayes_gradient + yerr_upper[1], proj_gradient + yerr_upper[2]) + 0.012
add_significance_bracket(ax, 1, 2, bracket_y, text='***')
ax.set_title('C', fontweight='bold', loc='left', fontsize=14)

plt.tight_layout()
plt.show()

print(f'Human gradient:      {human_gradient:+.3f}')
print(f'Bayesian:            {bayes_gradient:+.3f} ({bayes_gradient/human_gradient*100:.0f}% of human)')
print(f'Similarity Proj:     {proj_gradient:+.3f} ({proj_gradient/human_gradient*100:.0f}% of human)')
print(f'Scrambled:           {scrambled_gradient:+.3f}')
```

### Combined Figure 4

```{python}
#| label: fig4-combined
#| fig-cap: "Figure 4: Factor structure captures systematic generalization"
#| fig-width: 12
#| fig-height: 4

# Create combined figure for export
fig, axes = plt.subplots(1, 3, figsize=(12, 4))

# Panel A
ax = axes[0]
for i, mt in enumerate(['low', 'high']):
    offset = -width/2 if mt == 'low' else width/2
    rates = [model_rates[(qt, mt)] for qt in qt_keys]
    yerr = [[model_rates[(qt, mt)] - rate_cis[(qt, mt)][0] for qt in qt_keys],
            [rate_cis[(qt, mt)][1] - model_rates[(qt, mt)] for qt in qt_keys]]
    ax.bar(x + offset, rates, width, color=COLORS[mt], label=f'{mt.capitalize()} match',
           yerr=yerr, capsize=0, error_kw={'linewidth': 1.5})
ax.set_xticks(x)
ax.set_xticklabels(qt_labels)
ax.set_ylabel('Model P(shared)', fontweight='bold')
ax.set_ylim(0, 1.0)
ax.legend(frameon=False, loc='upper right')
ax.text(-0.15, 1.05, 'A', transform=ax.transAxes, fontsize=16, fontweight='bold')

# Panel B
ax = axes[1]
for qt in ['same_domain', 'different_domain']:
    subset = domain_df[domain_df['question_type'] == qt]
    color = COLORS['bayesian'] if qt == 'same_domain' else COLORS['scrambled']
    marker = 'o' if qt == 'same_domain' else 's'
    label = 'Same domain' if qt == 'same_domain' else 'Different domain'
    ax.scatter(subset['model_effect'], subset['human_effect'],
               c=color, marker=marker, s=80, edgecolors='white', linewidth=0.5, label=label)
ax.text(0.05, 0.95, f'r = {r:.2f}', transform=ax.transAxes, fontsize=12, fontweight='bold')
ax.plot(lims, lims, '--', color='gray', alpha=0.3, linewidth=1, zorder=0)
ax.set_xlim(lims)
ax.set_ylim(lims)
ax.set_xlabel('Model effect', fontweight='bold')
ax.set_ylabel('Human effect', fontweight='bold')
ax.legend(frameon=False, loc='lower right')
ax.text(-0.15, 1.05, 'B', transform=ax.transAxes, fontsize=16, fontweight='bold')

# Panel C
ax = axes[2]
ax.bar(range(len(gradients)), gradients, color=colors,
       yerr=[yerr_lower, yerr_upper], capsize=0, error_kw={'linewidth': 1.5})
ax.set_xticks(range(len(labels)))
ax.set_xticklabels(labels, fontsize=10)
ax.set_ylabel('Gradient (same − diff)', fontweight='bold')
ax.set_ylim(0, max([g + e for g, e in zip(gradients, yerr_upper)]) * 1.25)
add_significance_bracket(ax, 1, 2, bracket_y, text='***')
ax.text(-0.15, 1.05, 'C', transform=ax.transAxes, fontsize=16, fontweight='bold')

plt.tight_layout()
plt.savefig(base_dir / 'outputs' / 'figures' / 'figure4.pdf', dpi=300, bbox_inches='tight')
plt.show()
```

## 4. Figure 5: LLM Analysis

```{python}
#| label: llm-accuracy
# Load pre-computed LLM predictions
llm_accuracy = pd.read_csv(base_dir / 'data' / 'llm_results' / 'nochat_accuracy.csv')

print('LLM Accuracy by Question Type:\n')
for cat in ['matched', 'same_domain', 'different_domain']:
    subset = llm_accuracy[llm_accuracy['category'] == cat]
    if len(subset) > 0:
        acc = subset['prob_correct'].mean() * 100
        print(f'  {cat}: {acc:.1f}%')
```

### P(shared) Timecourse

```{python}
#| label: fig5
#| fig-cap: "Figure 5: LLM recovers structured generalization from conversation transcripts"
#| fig-width: 9
#| fig-height: 4

# Load timecourse data
pshared_df = pd.read_csv(base_dir / 'data' / 'llm_results' / 'pshared_timecourse.csv')
pshared_hl = pshared_df[pshared_df['match_type'].isin(['high', 'low'])].copy()

# Compute means
timecourse = pshared_hl.groupby(
    ['time_bin', 'bin_seconds', 'match_type', 'question_category']
)['predicted_agreement'].mean().reset_index()

# Human rates for comparison
chat_data = data[data['experiment'] == 'chat'].copy()
human_rates = {}
for mt in ['high', 'low']:
    for qt in ['observed', 'same_domain', 'different_domain']:
        subset = chat_data[(chat_data['match_type'] == mt) & (chat_data['question_type'] == qt)]
        human_rates[(mt, qt)] = subset['participant_binary_prediction'].mean()

# Plot
fig, axes = plt.subplots(1, 2, figsize=(9, 4), sharey=True)

cat_styles = {
    'matched': {'color': COLORS['focal'], 'lw': 2.5, 'marker': 'o', 'ms': 5, 'label': 'Focal (discussed)'},
    'same_domain': {'color': COLORS['same'], 'lw': 2, 'marker': 's', 'ms': 4, 'label': 'Same domain'},
    'different_domain': {'color': COLORS['diff'], 'lw': 1.5, 'marker': '^', 'ms': 4, 'label': 'Different domain'},
}
cat_to_human = {'matched': 'observed', 'same_domain': 'same_domain', 'different_domain': 'different_domain'}

for ax, (mt, title) in zip(axes, [('high', 'Shared Stance'), ('low', 'Opposing Stance')]):
    for cat in ['matched', 'same_domain', 'different_domain']:
        subset = timecourse[(timecourse['match_type'] == mt) & (timecourse['question_category'] == cat)]
        style = cat_styles[cat]
        ax.plot(subset['bin_seconds'], subset['predicted_agreement'],
                color=style['color'], linewidth=style['lw'],
                marker=style['marker'], markersize=style['ms'], label=style['label'])
        # Human dashed line
        ax.axhline(human_rates[(mt, cat_to_human[cat])], color=style['color'],
                   linestyle='--', linewidth=1.5, alpha=0.7)

    ax.set_xlabel('Conversation time (s)')
    ax.set_title(title, fontweight='bold')
    ax.set_ylim(0.1, 1.0)
    ax.set_xlim(-5, 195)

axes[1].legend(frameon=False, loc='upper right', fontsize=9)
axes[0].set_ylabel('P(shared)', fontweight='bold')
axes[0].text(-0.12, 1.05, 'A', transform=axes[0].transAxes, fontsize=16, fontweight='bold')
axes[1].text(-0.08, 1.05, 'B', transform=axes[1].transAxes, fontsize=16, fontweight='bold')

plt.tight_layout()
plt.savefig(base_dir / 'outputs' / 'figures' / 'figure5.pdf', dpi=300, bbox_inches='tight')
plt.show()
```

## 5. Summary

```{python}
#| label: summary
print('=== Key Results (no-chat condition) ===\n')

print('Generalization Gradient:')
print(f'  Human:               {human_gradient:+.3f}')
print(f'  Bayesian (k=5):      {bayes_gradient:+.3f} ({bayes_gradient/human_gradient*100:.0f}% of human)')
print(f'  Similarity Proj:     {proj_gradient:+.3f} ({proj_gradient/human_gradient*100:.0f}% of human)')
print(f'  Scrambled:           {scrambled_gradient:+.3f}')

print(f'\nDomain-level fit: r = {r:.2f}')
print(f'\n→ Bayesian model captures {bayes_gradient/human_gradient*100:.0f}% of human gradient')
print(f'→ Similarity Projection predicts ~0 gradient (uniform shifts)')
```
