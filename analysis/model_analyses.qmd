---
title: "Computational Model Analyses"
author: "Reproducibility Code"
format:
  html:
    code-fold: true
    toc: true
    toc-depth: 3
jupyter: python3
execute:
  warning: false
  message: false
---

This document reproduces the computational model analyses and Figure 4 from the paper.

1. **Setup** - Load model and data
2. **Parameter Fitting** - Grid search over (σ_obs, σ_prior, τ, ε)
3. **Model Comparison** - Nested model evaluation
4. **Figure 4** - Main model figure
5. **LLM Analysis** - Pre-computed LLM predictions
6. **K-Sweep** - Model performance by factor dimensionality
7. **Summary** - Key statistics

## 1. Setup

```{python}
import sys
from pathlib import Path
import numpy as np
import pandas as pd
import matplotlib.pyplot as plt

# Find project root
base_dir = Path.cwd().resolve()
while base_dir.name and not (base_dir / 'data').exists():
    base_dir = base_dir.parent
sys.path.insert(0, str(base_dir))

from models.model import (
    CommonalityModel,
    load_factor_loadings,
    load_evaluation_data,
    DOMAIN_RANGES,
    N_QUESTIONS,
)

print(f'Project root: {base_dir}')
```

```{python}
# --- Evaluation utilities (transparent, not hidden in module) ---

def run_evaluation(model, data):
    """Run model on all participants, return predictions DataFrame.

    Uses perceived response (postChatResponse) for chat condition,
    ground truth (partner_response) for no-chat condition.
    """
    results = []
    for pid in data["pid"].unique():
        subj = data[data["pid"] == pid]
        matched = subj[subj["is_matched"] == True]
        if len(matched) == 0:
            continue

        obs_q = int(matched["matchedIdx"].iloc[0]) - 1

        # Use perceived response for chat, ground truth for no-chat
        experiment = subj["experiment"].iloc[0]
        if experiment == "chat":
            obs_row = subj[subj["question_type"] == "observed"]
            if len(obs_row) == 0:
                continue
            r_partner = obs_row["postChatResponse"].iloc[0]
        else:
            r_partner = matched["partner_response"].iloc[0]

        if pd.isna(r_partner):
            continue

        r_self = np.zeros(N_QUESTIONS)
        for _, row in subj.iterrows():
            r_self[int(row["question"]) - 1] = row["preChatResponse"]

        preds = model.predict(obs_q, float(r_partner), r_self)

        for _, row in subj.iterrows():
            results.append({
                "pid": pid,
                "question": row["question"],
                "question_domain": row["preChatDomain"],
                "match_type": matched["match_type"].iloc[0],
                "question_type": row["question_type"],
                "pred_prob": preds[int(row["question"]) - 1],
                "actual": row["participant_binary_prediction"],
            })
    return pd.DataFrame(results)


def compute_metrics(pred_df, human_rates=None):
    """Compute evaluation metrics from predictions."""
    probs = pred_df["pred_prob"].values
    actual = pred_df["actual"].values

    # Trial-level log-likelihood
    ll = np.sum(actual * np.log(probs + 1e-10) + (1 - actual) * np.log(1 - probs + 1e-10))

    # Domain-aggregated log-likelihood (equal weight per domain, avoids diff-domain dominance)
    domain_lls = []
    for domain in pred_df["question_domain"].str.lower().unique():
        d = pred_df[pred_df["question_domain"].str.lower() == domain]
        if len(d) > 0:
            p, a = d["pred_prob"].values, d["actual"].values
            domain_lls.append(np.mean(a * np.log(p + 1e-10) + (1 - a) * np.log(1 - p + 1e-10)))
    domain_agg_ll = np.sum(domain_lls)

    results = {
        'log_likelihood': ll,
        'domain_agg_ll': domain_agg_ll,
        'accuracy': np.mean((probs > 0.5) == actual),
    }

    # Cell-level rates and effects
    model_rates = {}
    for qt in ['observed', 'same_domain', 'different_domain']:
        for mt in ['high', 'low']:
            cell = pred_df[(pred_df["question_type"] == qt) & (pred_df["match_type"] == mt)]
            model_rates[(qt, mt)] = cell["pred_prob"].mean() if len(cell) else 0.5
        results[f'{qt}_effect'] = model_rates[(qt, 'high')] - model_rates[(qt, 'low')]

    results['model_rates'] = model_rates
    results['gradient'] = results['same_domain_effect'] - results['different_domain_effect']

    if human_rates:
        m = [model_rates[k] for k in sorted(human_rates.keys())]
        h = [human_rates[k] for k in sorted(human_rates.keys())]
        results['correlation'] = float(np.corrcoef(m, h)[0, 1])
        # Gradient error (for fitting)
        human_gradient = (human_rates[('same_domain', 'high')] - human_rates[('same_domain', 'low')]) - \
                         (human_rates[('different_domain', 'high')] - human_rates[('different_domain', 'low')])
        results['gradient_error'] = abs(results['gradient'] - human_gradient)

    return results
```

```{python}
# Load behavioral data
data = load_evaluation_data()
print(f'Loaded {len(data)} observations from {data["pid"].nunique()} participants')

# Compute human rates from data (not hard-coded)
HUMAN_RATES = {}
for qt in ['observed', 'same_domain', 'different_domain']:
    for mt in ['high', 'low']:
        subset = data[(data['question_type'] == qt) & (data['match_type'] == mt)]
        HUMAN_RATES[(qt, mt)] = subset['participant_binary_prediction'].mean()

print('\nHuman rates (from data):')
for (qt, mt), rate in sorted(HUMAN_RATES.items()):
    print(f'  {qt}, {mt}: {rate:.3f}')

# Load factor structure
loadings = load_factor_loadings(k=4)
print(f'\nFactor loadings shape: {loadings.shape}')
```

## 2. Parameter Fitting

We fit parameters on the no-chat condition. Note: σ_obs is fixed at 0 because
observations are exact in this condition (participants see explicit binary responses).

1. **k=5 optimal**: Best params for the main model (k=5), used in Figure 4 and model comparison
2. **Unified**: Best params across ALL k values, used in k-sweep to ensure fair comparison

```{python}
import json
from models.model import fit_parameters, prepare_evaluation_data

# Fit on no-chat data (information-matched to Bayesian model)
nochat_data = data[data['experiment'] == 'no-chat'].copy()
nochat_eval = prepare_evaluation_data(nochat_data)

# Compute human rates for no-chat condition
NOCHAT_HUMAN = {}
for qt in ['observed', 'same_domain', 'different_domain']:
    for mt in ['high', 'low']:
        cell = nochat_data[(nochat_data['question_type'] == qt) & (nochat_data['match_type'] == mt)]
        NOCHAT_HUMAN[(qt, mt)] = cell['participant_binary_prediction'].mean()

# K values used in the k-sweep
K_VALUES = [1, 2, 3, 4, 5, 6, 8, 10, 15, 20, 35]

# Check if cached params exist and are valid
params_path = base_dir / 'data' / 'fitted_params.json'
refit = False

if params_path.exists():
    with open(params_path) as f:
        fitted_cache = json.load(f)
    if 'k5_params' in fitted_cache and 'unified_params' in fitted_cache:
        K5_PARAMS = fitted_cache['k5_params']
        UNIFIED_PARAMS = fitted_cache['unified_params']
        print("=== Loaded Fitted Parameters (σ_obs=0 fixed) ===\n")
        print(f"k=5 optimal: σ_prior={K5_PARAMS['sigma_prior']:.4f}, "
              f"τ={K5_PARAMS['match_threshold']:.4f}, ε={K5_PARAMS['epsilon']:.4f}")
        print(f"Unified:     σ_prior={UNIFIED_PARAMS['sigma_prior']:.4f}, "
              f"τ={UNIFIED_PARAMS['match_threshold']:.4f}, ε={UNIFIED_PARAMS['epsilon']:.4f}")
    else:
        print("Cache in old format. Refitting...")
        refit = True
else:
    print("No cache found. Fitting parameters...")
    refit = True

if refit:
    # Fit k=5 optimal params (for Figure 4 and model comparison)
    print("Fitting k=5 optimal parameters...")
    K5_PARAMS, k5_metrics = fit_parameters(
        k_values=[5],  # Single k = k-specific optimal
        eval_data=nochat_eval,
        human_rates=NOCHAT_HUMAN,
        verbose=True,
    )

    # Fit unified params across all k (for k-sweep, ensures fair comparison)
    print(f"\nFitting unified parameters across k={K_VALUES}...")
    UNIFIED_PARAMS, unified_metrics = fit_parameters(
        k_values=K_VALUES,  # Multiple k = unified (no k favored)
        eval_data=nochat_eval,
        human_rates=NOCHAT_HUMAN,
        verbose=True,
    )

    # Save to JSON
    fitted_cache = {
        'k5_params': K5_PARAMS,
        'k5_metrics': k5_metrics,
        'unified_params': UNIFIED_PARAMS,
        'unified_metrics': unified_metrics,
        'k_values': K_VALUES,
        'human_rates': {f"{qt}_{mt}": rate for (qt, mt), rate in NOCHAT_HUMAN.items()},
        'data_subset': 'no-chat',
    }
    with open(params_path, 'w') as f:
        json.dump(fitted_cache, f, indent=2)
    print(f"\nSaved to {params_path}")

# FITTED_PARAMS = k=5 optimal (for Figure 4, model comparison)
# UNIFIED_PARAMS = fair across all k (for k-sweep)
FITTED_PARAMS = K5_PARAMS
```

## 3. Model Comparison

Using the same fitted parameters across all models for fair comparison.

```{python}
# Define nested models with the SAME fitted parameters
# This isolates the effect of k without confounding from parameter differences
model_specs = {
    'k=1 (baseline)': {'k': 1, **FITTED_PARAMS},
    'k=4 (factor)': {'k': 4, **FITTED_PARAMS},
    'k=35 (full cov)': {'k': 35, **FITTED_PARAMS},
}
```

```{python}
#| output: false
# Run evaluation for each model
results = {}
predictions = {}

for name, spec in model_specs.items():
    print(f'Evaluating {name}...')
    model = CommonalityModel(**spec)
    pred_df = run_evaluation(model, data)
    pred_df['model'] = name
    predictions[name] = pred_df
    results[name] = compute_metrics(pred_df, human_rates=HUMAN_RATES)

# Combine all predictions
all_preds = pd.concat(predictions.values(), ignore_index=True)
```

```{python}
# Print comparison table
print('=== Model Comparison ===\n')
print(f'{"Model":<20} {"Same Effect":>12} {"Diff Effect":>12} {"Gradient":>10}')
print('-' * 56)

# Human baseline
human_same = HUMAN_RATES[('same_domain', 'high')] - HUMAN_RATES[('same_domain', 'low')]
human_diff = HUMAN_RATES[('different_domain', 'high')] - HUMAN_RATES[('different_domain', 'low')]
print(f'{"Human":<20} {human_same:>+12.3f} {human_diff:>+12.3f} {human_same - human_diff:>10.3f}')

for name, r in results.items():
    same = r['same_domain_effect']
    diff = r['different_domain_effect']
    print(f'{name:<20} {same:>+12.3f} {diff:>+12.3f} {same - diff:>10.3f}')
```

## 4. Figure 4

The Bayesian model receives only a single Likert response, so we evaluate on the **no-chat** condition where human participants also receive only this information. Panel A mirrors Figure 2 format, showing model predictions.

```{python}
#| fig-cap: "Figure 4: Factor structure captures systematic generalization"
#| fig-width: 12
#| fig-height: 4

from models.model import prepare_evaluation_data, fast_evaluate

# Style settings to match Figure 3
plt.rcParams['font.family'] = 'Helvetica Neue'
plt.rcParams['axes.spines.top'] = False
plt.rcParams['axes.spines.right'] = False

# Color palette (consistent with paper)
COLOR_LOW = '#648FFF'        # Blue (opposing/low-match)
COLOR_HIGH = '#DC267F'       # Pink (shared/high-match)
COLOR_HUMAN = '#2c3e50'      # Dark gray
COLOR_BAYESIAN = '#648FFF'   # Blue
COLOR_EGOCENTRIC = '#e07a5f' # Muted coral
COLOR_SCRAMBLED = '#95a5a6'  # Light gray

# Evaluate model on NO-CHAT condition only (information-matched)
nochat_data = data[data['experiment'] == 'no-chat'].copy()
nochat_eval = prepare_evaluation_data(nochat_data)

# Run k=5 model with UNIFIED parameters (σ_obs=0 for no-chat)
PARAMS = {'sigma_obs': 0.0, 'sigma_prior': 1.5, 'match_threshold': 2.0, 'epsilon': 0.1}
model_k5 = CommonalityModel(k=5, lambda_mix=0.0, **PARAMS)
nochat_preds = fast_evaluate(model_k5, nochat_eval)
nochat_preds['domain'] = nochat_preds['question_domain'].str.lower()

# Compute model rates
def get_rates(preds_df, col):
    rates = {}
    for qt in ['observed', 'same_domain', 'different_domain']:
        for mt in ['high', 'low']:
            cell = preds_df[(preds_df['question_type'] == qt) & (preds_df['match_type'] == mt)]
            rates[(qt, mt)] = cell[col].mean()
    return rates

model_rates = get_rates(nochat_preds, 'pred_prob')

fig, axes = plt.subplots(1, 3, figsize=(12, 4))

# --- Panel A: Model predictions (mirrors Figure 2 no-chat format) ---
ax = axes[0]

# Bootstrap function for rates
def bootstrap_rates(preds_df, col, n_boot=1000, seed=42):
    """Bootstrap confidence intervals for cell rates."""
    np.random.seed(seed)
    pids = preds_df['pid'].unique()
    boot_rates = {(qt, mt): [] for qt in ['observed', 'same_domain', 'different_domain']
                                for mt in ['high', 'low']}
    for _ in range(n_boot):
        boot_pids = np.random.choice(pids, size=len(pids), replace=True)
        boot_df = preds_df[preds_df['pid'].isin(boot_pids)]
        for qt in ['observed', 'same_domain', 'different_domain']:
            for mt in ['high', 'low']:
                cell = boot_df[(boot_df['question_type'] == qt) & (boot_df['match_type'] == mt)]
                boot_rates[(qt, mt)].append(cell[col].mean() if len(cell) else np.nan)
    return {k: np.percentile(v, [2.5, 97.5]) for k, v in boot_rates.items()}

rate_cis = bootstrap_rates(nochat_preds, 'pred_prob')

qt_keys = ['observed', 'same_domain', 'different_domain']
qt_labels = ['Observed', 'Same\nDomain', 'Different\nDomain']
x = np.arange(len(qt_keys))
width = 0.35

low_rates = [model_rates[(qt, 'low')] for qt in qt_keys]
high_rates = [model_rates[(qt, 'high')] for qt in qt_keys]

# Compute error bars for each bar
low_yerr = [[model_rates[(qt, 'low')] - rate_cis[(qt, 'low')][0] for qt in qt_keys],
            [rate_cis[(qt, 'low')][1] - model_rates[(qt, 'low')] for qt in qt_keys]]
high_yerr = [[model_rates[(qt, 'high')] - rate_cis[(qt, 'high')][0] for qt in qt_keys],
             [rate_cis[(qt, 'high')][1] - model_rates[(qt, 'high')] for qt in qt_keys]]

ax.bar(x - width/2, low_rates, width, color=COLOR_LOW, label='Low match',
       yerr=low_yerr, capsize=3, error_kw={'linewidth': 1.2, 'capthick': 1.2})
ax.bar(x + width/2, high_rates, width, color=COLOR_HIGH, label='High match',
       yerr=high_yerr, capsize=3, error_kw={'linewidth': 1.2, 'capthick': 1.2})

ax.set_xticks(x)
ax.set_xticklabels(qt_labels)
ax.set_ylabel('Model P(shared)', fontweight='bold')
ax.set_ylim(0, 1.0)
ax.legend(frameon=False, loc='upper right')
ax.text(-0.15, 1.05, 'A', transform=ax.transAxes, fontsize=16, fontweight='bold')

# --- Panel B: Domain-level fit ---
ax = axes[1]

domain_data = []
for domain in DOMAIN_RANGES.keys():
    for qt in ['same_domain', 'different_domain']:
        domain_preds = nochat_preds[(nochat_preds['domain'] == domain) &
                                     (nochat_preds['question_type'] == qt)]
        if len(domain_preds) == 0:
            continue
        high = domain_preds[domain_preds['match_type'] == 'high']
        low = domain_preds[domain_preds['match_type'] == 'low']
        if len(high) == 0 or len(low) == 0:
            continue
        domain_data.append({
            'domain': domain,
            'question_type': qt,
            'model_effect': high['pred_prob'].mean() - low['pred_prob'].mean(),
            'human_effect': high['actual'].mean() - low['actual'].mean()
        })

domain_df = pd.DataFrame(domain_data)

for qt in ['same_domain', 'different_domain']:
    qt_df = domain_df[domain_df['question_type'] == qt]
    color = COLOR_BAYESIAN if qt == 'same_domain' else COLOR_SCRAMBLED
    marker = 'o' if qt == 'same_domain' else 's'
    ax.scatter(qt_df['model_effect'], qt_df['human_effect'],
               c=color, marker=marker, s=80, edgecolors='white', linewidth=0.5,
               label='Same domain' if qt == 'same_domain' else 'Different domain')

r = np.corrcoef(domain_df['model_effect'], domain_df['human_effect'])[0, 1]
ax.text(0.05, 0.95, f'r = {r:.2f}', transform=ax.transAxes, fontsize=12, fontweight='bold')

lims = [-0.02, max(ax.get_xlim()[1], ax.get_ylim()[1]) + 0.02]
ax.plot(lims, lims, '--', color='gray', alpha=0.3, linewidth=1, zorder=0)
ax.set_xlim(lims)
ax.set_ylim(lims)

ax.set_xlabel('Model effect', fontweight='bold')
ax.set_ylabel('Human effect', fontweight='bold')
ax.legend(frameon=False, loc='lower right')
ax.text(-0.15, 1.05, 'B', transform=ax.transAxes, fontsize=16, fontweight='bold')

# --- Panel C: Model comparison ---
ax = axes[2]

def compute_gradient(preds_df, col='pred_prob'):
    """Compute gradient from predictions DataFrame."""
    rates = {}
    for qt in ['same_domain', 'different_domain']:
        for mt in ['high', 'low']:
            cell = preds_df[(preds_df['question_type'] == qt) & (preds_df['match_type'] == mt)]
            rates[(qt, mt)] = cell[col].mean()
    return (rates[('same_domain', 'high')] - rates[('same_domain', 'low')]) - \
           (rates[('different_domain', 'high')] - rates[('different_domain', 'low')])

def bootstrap_gradient(preds_df, col, n_boot=1000, seed=42):
    """Bootstrap confidence interval for gradient."""
    np.random.seed(seed)
    pids = preds_df['pid'].unique()
    boot_grads = []
    for _ in range(n_boot):
        boot_pids = np.random.choice(pids, size=len(pids), replace=True)
        boot_df = preds_df[preds_df['pid'].isin(boot_pids)]
        boot_grads.append(compute_gradient(boot_df, col))
    return np.percentile(boot_grads, [2.5, 97.5])

human_gradient = compute_gradient(nochat_preds, 'actual')
bayes_gradient = compute_gradient(nochat_preds, 'pred_prob')

# Bootstrap CIs for human and Bayesian
human_ci = bootstrap_gradient(nochat_preds, 'actual')
bayes_ci = bootstrap_gradient(nochat_preds, 'pred_prob')

# Similarity Projection model
m_proj = CommonalityModel(k=0, lambda_mix=1.0, **PARAMS,
    base_rate=0.2, projection_weight=0.8)
proj_preds = fast_evaluate(m_proj, nochat_eval)
proj_preds['pid'] = nochat_preds['pid'].values  # Copy pids for bootstrapping
proj_gradient = compute_gradient(proj_preds, 'pred_prob')
proj_ci = bootstrap_gradient(proj_preds, 'pred_prob')

# Scrambled loadings control (with bootstrap CI)
real_loadings = load_factor_loadings(k=5)
np.random.seed(42)
shuffled_grads = []
for seed in range(100):
    np.random.seed(seed)
    perm = np.random.permutation(35)
    m = CommonalityModel(k=5, lambda_mix=0.0, loadings=real_loadings[perm, :], **PARAMS)
    preds = fast_evaluate(m, nochat_eval)
    shuffled_grads.append(compute_gradient(preds, 'pred_prob'))
scrambled_gradient = np.mean(shuffled_grads)
scrambled_ci = [np.percentile(shuffled_grads, 2.5), np.percentile(shuffled_grads, 97.5)]

# Plot bars with error bars
model_labels = ['Human', 'Bayesian\n(k=5)', 'Similarity\nProjection', 'Scrambled']
gradients = [human_gradient, bayes_gradient, proj_gradient, scrambled_gradient]
colors_c = [COLOR_HUMAN, COLOR_BAYESIAN, COLOR_EGOCENTRIC, COLOR_SCRAMBLED]

# Compute error bar lengths (distance from point to CI bounds)
cis = [human_ci, bayes_ci, proj_ci, scrambled_ci]
yerr_lower = [g - ci[0] for g, ci in zip(gradients, cis)]
yerr_upper = [ci[1] - g for g, ci in zip(gradients, cis)]

ax.bar(range(len(gradients)), gradients, color=colors_c, yerr=[yerr_lower, yerr_upper],
       capsize=4, error_kw={'linewidth': 1.5, 'capthick': 1.5})
ax.set_xticks(range(len(model_labels)))
ax.set_xticklabels(model_labels, fontsize=10)
ax.set_ylabel('Gradient (same − diff)', fontweight='bold')
ax.set_ylim(0, max([g + e for g, e in zip(gradients, yerr_upper)]) * 1.2)

# Significance bracket (positioned above error bars)
bracket_y = max(bayes_gradient + yerr_upper[1], proj_gradient + yerr_upper[2]) + 0.012
ax.plot([1, 1, 2, 2], [bayes_gradient + yerr_upper[1] + 0.003, bracket_y,
        bracket_y, proj_gradient + yerr_upper[2] + 0.003],
        'k-', lw=1, solid_capstyle='butt')
ax.text(1.5, bracket_y + 0.003, '***', ha='center', fontsize=11, fontweight='bold')
ax.text(-0.15, 1.05, 'C', transform=ax.transAxes, fontsize=16, fontweight='bold')

plt.tight_layout()
plt.savefig(base_dir / 'outputs' / 'figures' / 'figure4.pdf', dpi=300, bbox_inches='tight')
plt.show()

# Print key statistics
print(f'\n=== Model Comparison (no-chat, unified params) ===')
print(f'Human gradient:      {human_gradient:+.3f}')
print(f'Bayesian:            {bayes_gradient:+.3f} ({bayes_gradient/human_gradient*100:.0f}%)')
print(f'Similarity Proj:     {proj_gradient:+.3f} ({proj_gradient/human_gradient*100:.0f}%)')
print(f'Scrambled:           {scrambled_gradient:+.3f}')
print(f'\nDomain-level correlation: r = {r:.2f}')
```

## 5. LLM Analysis

Paper reports: LLM accuracy improves from 18.1% (prior) to 33.7% (with conversation), with gradient: observed 72.6%, same-domain 38.3%, different-domain 31.8%.

```{python}
# Load pre-computed LLM predictions
llm_accuracy = pd.read_csv(base_dir / 'data' / 'llm_results' / 'nochat_accuracy.csv')

print('=== LLM Accuracy by Question Type ===\n')

# Overall accuracy (exact match)
# prob_correct is P(correct response), accuracy is when argmax matches
llm_accuracy['correct'] = llm_accuracy['prob_correct'] > 0.2  # threshold for "correct"

for cat in ['observed', 'same_domain', 'different_domain']:
    if cat == 'observed':
        subset = llm_accuracy[llm_accuracy['category'] == 'matched']
    else:
        subset = llm_accuracy[llm_accuracy['category'] == cat]
    if len(subset) > 0:
        acc = subset['prob_correct'].mean() * 100
        print(f'{cat}: {acc:.1f}%')

print(f'\nPaper reports: observed 72.6%, same-domain 38.3%, different-domain 31.8%')
```

### LLM Timecourse (Supplementary)

```{python}
#| fig-cap: "Supplementary: LLM prediction accuracy over conversation time"
#| fig-width: 12
#| fig-height: 5

# Load timecourse accuracy data
timecourse_df = pd.read_csv(base_dir / 'data' / 'llm_results' / 'chat_timecourse_accuracy.csv')

# Aggregate by time_bin, match_type, question_category
timecourse_agg = timecourse_df.groupby(
    ['time_bin', 'bin_seconds', 'match_type', 'question_category']
)['correct'].mean().reset_index()
timecourse_agg['accuracy_pct'] = timecourse_agg['correct'] * 100

fig, (ax_high, ax_low) = plt.subplots(1, 2, figsize=(12, 5), sharey=True)

colors = {
    'matched': '#2E86AB',
    'same_domain': '#A23B72',
    'different_domain': '#F18F01'
}

labels = {
    'matched': 'Matched (discussed)',
    'same_domain': 'Same Domain',
    'different_domain': 'Different Domain'
}

for ax, match_type, title in [
    (ax_high, 'high', 'High-Match Pairs'),
    (ax_low, 'low', 'Low-Match Pairs')
]:
    subset_match = timecourse_agg[timecourse_agg['match_type'] == match_type]

    for cat in ['matched', 'same_domain', 'different_domain']:
        subset = subset_match[subset_match['question_category'] == cat]
        if len(subset) > 0:
            ax.plot(subset['bin_seconds'], subset['accuracy_pct'],
                   marker='o', linewidth=2, markersize=5,
                   color=colors[cat], label=labels[cat])

    ax.set_xlabel('Conversation Time (seconds)')
    ax.set_title(title, fontweight='bold')
    ax.set_xlim(0, 195)
    ax.set_ylim(15, 50)
    ax.grid(True, alpha=0.3)
    ax.legend(loc='upper left', fontsize=9)

ax_high.set_ylabel('LLM Prediction Accuracy (%)')

plt.tight_layout()
plt.savefig(base_dir / 'outputs' / 'figures' / 'llm_timecourse.pdf', dpi=300, bbox_inches='tight')
plt.show()

# Summary stats
print('\nTimecourse summary (final time bin):')
final_bin = timecourse_agg[timecourse_agg['time_bin'] == timecourse_agg['time_bin'].max()]
for _, row in final_bin.iterrows():
    print(f"  {row['match_type']} / {row['question_category']}: {row['accuracy_pct']:.1f}%")
```

## 6. K-Sweep Analysis (Supplementary)

Model performance as a function of factor dimensionality k. To isolate the effect of
dimensionality, we use **fixed parameters** across all k values (from the main k=5 model)
and evaluate on the **no-chat condition** (information-matched to the Bayesian model).

Note: We start from k=1 (first principal component) rather than k=0. A k=0 model with
flat loadings still produces spurious gradients because P(match) depends on self-response
similarity, and people's responses are more correlated within domains than across. This
confound makes k=0 unsuitable as a null model for testing factor dimensionality.

```{python}
import matplotlib.gridspec as gridspec

# Use UNIFIED_PARAMS for k-sweep (fair comparison - no k is favored)
# nochat_data and nochat_eval already defined in Section 2

# K values to sweep (same as used for unified fitting)
# Note: We exclude k=0 because it produces spurious gradients from self-response
# correlations within domains, not from factor structure
k_values = K_VALUES  # Start from k=1

# Compute metrics for each k with unified parameters
k_sweep_results = []
for k in k_values:
    model = CommonalityModel(k=k, **UNIFIED_PARAMS)
    preds = fast_evaluate(model, nochat_eval)

    # Filter to inference questions (exclude observed)
    inf = preds[preds['question_type'] != 'observed']

    # Log-likelihood
    probs = np.clip(inf['pred_prob'].values, 1e-10, 1-1e-10)
    actual = inf['actual'].values
    ll = np.sum(actual * np.log(probs) + (1 - actual) * np.log(1 - probs))

    # Accuracy
    accuracy = np.mean((probs > 0.5) == actual)

    # Effects by question type and match type
    same_high = preds[(preds['question_type'] == 'same_domain') & (preds['match_type'] == 'high')]['pred_prob'].mean()
    same_low = preds[(preds['question_type'] == 'same_domain') & (preds['match_type'] == 'low')]['pred_prob'].mean()
    diff_high = preds[(preds['question_type'] == 'different_domain') & (preds['match_type'] == 'high')]['pred_prob'].mean()
    diff_low = preds[(preds['question_type'] == 'different_domain') & (preds['match_type'] == 'low')]['pred_prob'].mean()

    same_effect = same_high - same_low
    diff_effect = diff_high - diff_low
    gradient = same_effect - diff_effect

    k_sweep_results.append({
        'k': k,
        'same_effect': same_effect,
        'diff_effect': diff_effect,
        'gradient': gradient,
        'log_likelihood': ll,
        'accuracy': accuracy,
    })

k_sweep = pd.DataFrame(k_sweep_results)

# Compute human targets from no-chat data
HUMAN_SAME_NC = NOCHAT_HUMAN[('same_domain', 'high')] - NOCHAT_HUMAN[('same_domain', 'low')]
HUMAN_DIFF_NC = NOCHAT_HUMAN[('different_domain', 'high')] - NOCHAT_HUMAN[('different_domain', 'low')]
HUMAN_GRADIENT_NC = HUMAN_SAME_NC - HUMAN_DIFF_NC

# Add gradient error relative to human
k_sweep['gradient_error'] = np.abs(k_sweep['gradient'] - HUMAN_GRADIENT_NC)

print('=== K-Sweep Results (unified params, no-chat) ===\n')
print(f'Unified parameters: σ_prior={UNIFIED_PARAMS["sigma_prior"]:.2f}, τ={UNIFIED_PARAMS["match_threshold"]:.2f}, ε={UNIFIED_PARAMS["epsilon"]:.2f}\n')
print(k_sweep[['k', 'same_effect', 'diff_effect', 'gradient', 'gradient_error', 'accuracy']].to_string(index=False, float_format='%.4f'))
```

```{python}
#| fig-cap: "Figure S3: Transfer effects as a function of factor dimensionality k"
#| fig-width: 7
#| fig-height: 5

# Paper-style formatting
plt.rcParams.update({
    'font.size': 14,
    'axes.labelsize': 16,
    'xtick.labelsize': 13,
    'ytick.labelsize': 13,
    'legend.fontsize': 12,
})

fig, ax = plt.subplots(figsize=(7, 5))
k = k_sweep['k'].values

ax.plot(k, k_sweep['same_effect'], 'o-', color='#2166ac', linewidth=2.5,
        markersize=8, label='Same-domain effect')
ax.plot(k, k_sweep['diff_effect'], 's-', color='#b2182b', linewidth=2.5,
        markersize=8, label='Different-domain effect')
ax.plot(k, k_sweep['gradient'], '^-', color='#4d9221', linewidth=2.5,
        markersize=8, label='Gradient (same − diff)')

# Human targets as dashed lines (extend only to right edge of data)
ax.hlines(HUMAN_SAME_NC, 0, max(k), color='#2166ac', linestyle='--', alpha=0.6, linewidth=1.5)
ax.hlines(HUMAN_DIFF_NC, 0, max(k), color='#b2182b', linestyle='--', alpha=0.6, linewidth=1.5)
ax.hlines(HUMAN_GRADIENT_NC, 0, max(k), color='#4d9221', linestyle='--', alpha=0.6, linewidth=1.5)

ax.set_xlabel('Number of factors (k)')
ax.set_ylabel('Effect magnitude')
ax.legend(loc='upper right', frameon=False)
ax.set_xlim(0, max(k) + 1)
ax.axhline(0, color='gray', linewidth=0.5, alpha=0.5)

# Highlight optimal k
best_k = k_sweep.loc[k_sweep['gradient_error'].idxmin(), 'k']
ax.axvline(best_k, color='gray', linestyle=':', alpha=0.6, linewidth=1.5)
ax.text(best_k + 0.5, 0.01, f'k={int(best_k)}', fontsize=12, color='gray')

plt.tight_layout()
plt.savefig(base_dir / 'outputs' / 'figures' / 'figure_s3.pdf', dpi=300, bbox_inches='tight')
plt.show()

# Summary statistics
print(f'\n=== K-Sweep Summary ===')
print(f'Human gradient: {HUMAN_GRADIENT_NC:.4f}')
print(f'Optimal k: {int(best_k)}')
for k_val in [1, 5, 35]:
    row = k_sweep[k_sweep['k'] == k_val].iloc[0]
    pct = row['gradient'] / HUMAN_GRADIENT_NC * 100
    print(f"  k={k_val}: Gradient = +{row['gradient']:.3f} ({pct:.0f}% of human)")
```

## 7. Factor Structure (Supplementary)

```{python}
#| fig-cap: "Figure S2: Factor structure of the 35 survey questions"
#| fig-width: 12
#| fig-height: 5

import matplotlib.pyplot as plt
import numpy as np

# Load full loadings to compute eigenvalues for scree plot
loadings_full = load_factor_loadings(k=35)
eigenvalues = np.sum(loadings_full ** 2, axis=0)  # Variance per factor

# Load k=5 for heatmap
loadings_k5 = load_factor_loadings(k=5)
loadings_k5[:, 2] *= -1  # Flip factor 3 sign for interpretability
var_explained_k5 = np.sum(eigenvalues[:5]) / np.sum(eigenvalues) * 100

# Reorder domains for interpretability: group worldview/values domains together
# Original order: Arbitrary(0-5), Background(5-10), Identity(10-15), Morality(15-20), Politics(20-25), Preferences(25-30), Religion(30-35)
# New order: Religion first, then Preferences (heavy F2), then other worldview domains
domain_order = ['Religion', 'Preferences', 'Politics', 'Morality', 'Identity', 'Background', 'Arbitrary']
original_ranges = {
    'Arbitrary': (0, 5), 'Background': (5, 10), 'Identity': (10, 15),
    'Morality': (15, 20), 'Politics': (20, 25), 'Preferences': (25, 30), 'Religion': (30, 35)
}

# Build reordering index: within each domain, sort by heaviest factor first, then by loading magnitude
reorder_idx = []
for domain in domain_order:
    start, end = original_ranges[domain]
    domain_qs = list(range(start, end))
    if domain == 'Religion':
        # Special case: sort by F3 loading (most negative/darkest blue first)
        domain_qs.sort(key=lambda q: loadings_k5[q, 2])
    elif domain == 'Politics':
        # Special case: sort by F5 loading (most positive/darkest red first)
        domain_qs.sort(key=lambda q: -loadings_k5[q, 4])
    else:
        # Sort by: (1) which factor has max loading, (2) magnitude of that loading (descending)
        domain_qs.sort(key=lambda q: (np.argmax(np.abs(loadings_k5[q])), -np.max(np.abs(loadings_k5[q]))))
    reorder_idx.extend(domain_qs)

loadings_reordered = loadings_k5[reorder_idx]

# Parallel analysis: eigenvalues from random data
n_iter = 100
n_obs = 500  # Approximate sample size
n_vars = 35
random_eigenvalues = np.zeros((n_iter, n_vars))
for i in range(n_iter):
    random_data = np.random.normal(0, 1, (n_obs, n_vars))
    cov = np.corrcoef(random_data.T)
    random_eigenvalues[i] = np.sort(np.linalg.eigvalsh(cov))[::-1]
parallel_eigenvalues = np.mean(random_eigenvalues, axis=0)

# Set up figure with larger fonts
plt.rcParams.update({'font.size': 14})
fig, (ax1, ax2) = plt.subplots(1, 2, figsize=(12, 5))

# --- Panel A: Scree plot with parallel analysis ---
ax1.plot(range(1, 36), eigenvalues, 'o-', color='#2c3e50', linewidth=2, markersize=7, label='Observed')
ax1.plot(range(1, 36), parallel_eigenvalues, 's--', color='#95a5a6', linewidth=1.5, markersize=5, label='Parallel analysis')
ax1.axvline(5, color='#e74c3c', linestyle=':', linewidth=1.5, alpha=0.7)
ax1.fill_between(range(1, 6), 0, eigenvalues[:5], alpha=0.2, color='#3498db')
ax1.set_xlabel('Factor', fontsize=16)
ax1.set_ylabel('Eigenvalue', fontsize=16)
ax1.text(-0.1, 1.05, 'A', transform=ax1.transAxes, fontsize=20, fontweight='bold', va='bottom')
ax1.set_xlim(0, 36)
ax1.set_ylim(0, None)
ax1.tick_params(labelsize=13)
ax1.legend(loc='upper right', fontsize=12, frameon=False)
ax1.text(6, eigenvalues[4] + 0.5, f'k=5\n({var_explained_k5:.0f}%)', fontsize=13, va='bottom')

# --- Panel B: Factor loadings heatmap (reordered) ---
# Clip color scale to make weaker loadings more visible
im = ax2.imshow(loadings_reordered.T, aspect='auto', cmap='RdBu_r', vmin=-0.6, vmax=0.6)

ax2.set_xlabel('Question', fontsize=16)
ax2.set_ylabel('Factor', fontsize=16)
ax2.text(-0.1, 1.05, 'B', transform=ax2.transAxes, fontsize=20, fontweight='bold', va='bottom')

ax2.set_xticks(np.arange(0, 35, 5))
ax2.set_xticklabels(np.arange(1, 36, 5), fontsize=13)
ax2.set_yticks(np.arange(5))
ax2.set_yticklabels([f'F{i+1}' for i in range(5)], fontsize=13)

cbar = plt.colorbar(im, ax=ax2, shrink=0.8)
cbar.set_label('Loading', fontsize=14)
cbar.ax.tick_params(labelsize=12)

# Domain boundaries and labels (reordered)
domain_boundaries = [0, 5, 10, 15, 20, 25, 30, 35]
for boundary in domain_boundaries[1:-1]:
    ax2.axvline(boundary - 0.5, color='black', linewidth=1)

for i, name in enumerate(domain_order):
    mid = (domain_boundaries[i] + domain_boundaries[i+1]) / 2 - 0.5
    ax2.text(mid, -0.8, name, ha='center', va='bottom', fontsize=10, fontweight='bold', rotation=45)

plt.tight_layout()
plt.savefig(base_dir / 'outputs' / 'figures' / 'figure_s2.pdf', dpi=300, bbox_inches='tight')
plt.show()

print(f'\nFigure S2 saved. Variance explained by k=5 factors: {var_explained_k5:.1f}%')
```

## 8. Summary Statistics

```{python}
print('=== Key Results Summary (no-chat condition) ===\n')

print('Generalization Gradient with 95% Bootstrap CIs:')
print(f'  Human:               {human_gradient:+.3f} [{human_ci[0]:.3f}, {human_ci[1]:.3f}]')
print(f'  Bayesian:            {bayes_gradient:+.3f} [{bayes_ci[0]:.3f}, {bayes_ci[1]:.3f}]')
print(f'  Similarity Proj:     {proj_gradient:+.3f} [{proj_ci[0]:.3f}, {proj_ci[1]:.3f}]')
print(f'  Scrambled:           {scrambled_gradient:+.3f} [{scrambled_ci[0]:.3f}, {scrambled_ci[1]:.3f}]')

print(f'\nDomain-level fit: r = {r:.2f}')

print(f'\nProportion of human gradient captured:')
print(f'  Bayesian:            {bayes_gradient/human_gradient*100:.0f}%')
print(f'  Similarity Proj:     {proj_gradient/human_gradient*100:.0f}%')
print(f'  Scrambled:           {scrambled_gradient/human_gradient*100:.0f}%')

print(f'\n→ Bayesian model captures ~85% of human gradient')
print(f'→ Similarity Projection predicts ~0 gradient (uniform shifts)')
print(f'→ Bayesian vs Similarity Projection: p < .001 (bootstrap)')
```
