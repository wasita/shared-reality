---
title: "Computational Model Analyses"
author: "Reproducibility Code"
format:
  html:
    code-fold: true
    toc: true
    toc-depth: 3
jupyter: python3
execute:
  warning: false
  message: false
---

This document reproduces the computational model analyses and Figure 4 from the paper.

1. **Setup** - Load model and data
2. **Parameter Fitting** - Grid search over (σ_obs, σ_prior, τ, ε)
3. **Model Comparison** - Nested model evaluation
4. **Figure 4** - Main model figure
5. **LLM Analysis** - Pre-computed LLM predictions
6. **K-Sweep** - Model performance by factor dimensionality
7. **Summary** - Key statistics

## 1. Setup

```{python}
import sys
from pathlib import Path
import numpy as np
import pandas as pd
import matplotlib.pyplot as plt

# Find project root
base_dir = Path.cwd().resolve()
while base_dir.name and not (base_dir / 'data').exists():
    base_dir = base_dir.parent
sys.path.insert(0, str(base_dir))

from models.model import (
    CommonalityModel,
    load_factor_loadings,
    load_evaluation_data,
    DOMAIN_RANGES,
    N_QUESTIONS,
)

print(f'Project root: {base_dir}')
```

```{python}
# --- Evaluation utilities (transparent, not hidden in module) ---

def run_evaluation(model, data):
    """Run model on all participants, return predictions DataFrame.

    Uses perceived response (postChatResponse) for chat condition,
    ground truth (partner_response) for no-chat condition.
    """
    results = []
    for pid in data["pid"].unique():
        subj = data[data["pid"] == pid]
        matched = subj[subj["is_matched"] == True]
        if len(matched) == 0:
            continue

        obs_q = int(matched["matchedIdx"].iloc[0]) - 1

        # Use perceived response for chat, ground truth for no-chat
        experiment = subj["experiment"].iloc[0]
        if experiment == "chat":
            obs_row = subj[subj["question_type"] == "observed"]
            if len(obs_row) == 0:
                continue
            r_partner = obs_row["postChatResponse"].iloc[0]
        else:
            r_partner = matched["partner_response"].iloc[0]

        if pd.isna(r_partner):
            continue

        r_self = np.zeros(N_QUESTIONS)
        for _, row in subj.iterrows():
            r_self[int(row["question"]) - 1] = row["preChatResponse"]

        preds = model.predict(obs_q, float(r_partner), r_self)

        for _, row in subj.iterrows():
            results.append({
                "pid": pid,
                "question": row["question"],
                "question_domain": row["preChatDomain"],
                "match_type": matched["match_type"].iloc[0],
                "question_type": row["question_type"],
                "pred_prob": preds[int(row["question"]) - 1],
                "actual": row["participant_binary_prediction"],
            })
    return pd.DataFrame(results)


def compute_metrics(pred_df, human_rates=None):
    """Compute evaluation metrics from predictions."""
    probs = pred_df["pred_prob"].values
    actual = pred_df["actual"].values

    # Trial-level log-likelihood
    ll = np.sum(actual * np.log(probs + 1e-10) + (1 - actual) * np.log(1 - probs + 1e-10))

    # Domain-aggregated log-likelihood (equal weight per domain, avoids diff-domain dominance)
    domain_lls = []
    for domain in pred_df["question_domain"].str.lower().unique():
        d = pred_df[pred_df["question_domain"].str.lower() == domain]
        if len(d) > 0:
            p, a = d["pred_prob"].values, d["actual"].values
            domain_lls.append(np.mean(a * np.log(p + 1e-10) + (1 - a) * np.log(1 - p + 1e-10)))
    domain_agg_ll = np.sum(domain_lls)

    results = {
        'log_likelihood': ll,
        'domain_agg_ll': domain_agg_ll,
        'accuracy': np.mean((probs > 0.5) == actual),
    }

    # Cell-level rates and effects
    model_rates = {}
    for qt in ['observed', 'same_domain', 'different_domain']:
        for mt in ['high', 'low']:
            cell = pred_df[(pred_df["question_type"] == qt) & (pred_df["match_type"] == mt)]
            model_rates[(qt, mt)] = cell["pred_prob"].mean() if len(cell) else 0.5
        results[f'{qt}_effect'] = model_rates[(qt, 'high')] - model_rates[(qt, 'low')]

    results['model_rates'] = model_rates
    results['gradient'] = results['same_domain_effect'] - results['different_domain_effect']

    if human_rates:
        m = [model_rates[k] for k in sorted(human_rates.keys())]
        h = [human_rates[k] for k in sorted(human_rates.keys())]
        results['correlation'] = float(np.corrcoef(m, h)[0, 1])
        # Gradient error (for fitting)
        human_gradient = (human_rates[('same_domain', 'high')] - human_rates[('same_domain', 'low')]) - \
                         (human_rates[('different_domain', 'high')] - human_rates[('different_domain', 'low')])
        results['gradient_error'] = abs(results['gradient'] - human_gradient)

    return results
```

```{python}
# Load behavioral data
data = load_evaluation_data()
print(f'Loaded {len(data)} observations from {data["pid"].nunique()} participants')

# Compute human rates from data (not hard-coded)
HUMAN_RATES = {}
for qt in ['observed', 'same_domain', 'different_domain']:
    for mt in ['high', 'low']:
        subset = data[(data['question_type'] == qt) & (data['match_type'] == mt)]
        HUMAN_RATES[(qt, mt)] = subset['participant_binary_prediction'].mean()

print('\nHuman rates (from data):')
for (qt, mt), rate in sorted(HUMAN_RATES.items()):
    print(f'  {qt}, {mt}: {rate:.3f}')

# Load factor structure
loadings = load_factor_loadings(k=4)
print(f'\nFactor loadings shape: {loadings.shape}')
```

## 2. Parameter Fitting

We fit model parameters (σ_obs, σ_prior, τ, ε) by grid search, minimizing deviation from human transfer effects. This is done separately for each k to ensure fair model comparison.

Parameters are pre-computed and cached in `data/fitted_params.json`.

```{python}
import json

# Load pre-computed fitted parameters
with open(base_dir / 'data' / 'fitted_params.json') as f:
    fitted_cache = json.load(f)

# Extract params for main comparison models
fitted_params = {
    int(k): fitted_cache['bayesian'][k]['params']
    for k in ['1', '4', '35']
}

# Display fitted parameters
print("=== Fitted Parameters (from cache) ===\n")
for k, params in sorted(fitted_params.items()):
    print(f"k={k}: σ_obs={params['sigma_obs']}, σ_prior={params['sigma_prior']}, "
          f"τ={params['match_threshold']}, ε={params['epsilon']}")
```

## 3. Model Comparison

Using fitted parameters for fair comparison.

```{python}
# Define nested models with fitted parameters
model_specs = {
    'k=1 (baseline)': {'k': 1, **fitted_params[1]},
    'k=4 (factor)': {'k': 4, **fitted_params[4]},
    'k=35 (full cov)': {'k': 35, **fitted_params[35]},
}
```

```{python}
#| output: false
# Run evaluation for each model
results = {}
predictions = {}

for name, spec in model_specs.items():
    print(f'Evaluating {name}...')
    model = CommonalityModel(**spec)
    pred_df = run_evaluation(model, data)
    pred_df['model'] = name
    predictions[name] = pred_df
    results[name] = compute_metrics(pred_df, human_rates=HUMAN_RATES)

# Combine all predictions
all_preds = pd.concat(predictions.values(), ignore_index=True)
```

```{python}
# Print comparison table
print('=== Model Comparison ===\n')
print(f'{"Model":<20} {"Same Effect":>12} {"Diff Effect":>12} {"Gradient":>10}')
print('-' * 56)

# Human baseline
human_same = HUMAN_RATES[('same_domain', 'high')] - HUMAN_RATES[('same_domain', 'low')]
human_diff = HUMAN_RATES[('different_domain', 'high')] - HUMAN_RATES[('different_domain', 'low')]
print(f'{"Human":<20} {human_same:>+12.3f} {human_diff:>+12.3f} {human_same - human_diff:>10.3f}')

for name, r in results.items():
    same = r['same_domain_effect']
    diff = r['different_domain_effect']
    print(f'{name:<20} {same:>+12.3f} {diff:>+12.3f} {same - diff:>10.3f}')
```

## 4. Figure 4

The Bayesian model receives only a single Likert response, so we evaluate on the **no-chat** condition where human participants also receive only this information.

```{python}
#| fig-cap: "Figure 4: Factor structure captures systematic generalization"
#| fig-width: 12
#| fig-height: 4

from models.model import prepare_evaluation_data, fast_evaluate

# Evaluate model on NO-CHAT condition only (information-matched)
nochat_data = data[data['experiment'] == 'no-chat'].copy()
nochat_eval = prepare_evaluation_data(nochat_data)

# Run k=5 model (Bayesian, λ=0) with paper's parameters
model_k5 = CommonalityModel(
    k=5, lambda_mix=0.0,
    sigma_obs=0.3, sigma_prior=2.0, match_threshold=1.5, epsilon=0.2
)
nochat_preds = fast_evaluate(model_k5, nochat_eval)
nochat_preds['domain'] = nochat_preds['question_domain'].str.lower()

# Compute human rates for no-chat condition
NOCHAT_HUMAN = {}
for qt in ['observed', 'same_domain', 'different_domain']:
    for mt in ['high', 'low']:
        cell = nochat_data[(nochat_data['question_type'] == qt) & (nochat_data['match_type'] == mt)]
        NOCHAT_HUMAN[(qt, mt)] = cell['participant_binary_prediction'].mean()

fig, axes = plt.subplots(1, 3, figsize=(12, 4))

# --- Panel A: Generalization Gradient (effects) ---
ax = axes[0]

qt_labels = ['Observed', 'Same\nDomain', 'Different\nDomain']
qt_keys = ['observed', 'same_domain', 'different_domain']
x = np.arange(len(qt_keys))
width = 0.35

# Compute effects (high - low) for human and model
human_effects = [NOCHAT_HUMAN[(qt, 'high')] - NOCHAT_HUMAN[(qt, 'low')] for qt in qt_keys]

model_low, model_high = [], []
for qt in qt_keys:
    qt_preds = nochat_preds[nochat_preds['question_type'] == qt]
    model_high.append(qt_preds[qt_preds['match_type'] == 'high']['pred_prob'].mean())
    model_low.append(qt_preds[qt_preds['match_type'] == 'low']['pred_prob'].mean())
model_effects = [h - l for h, l in zip(model_high, model_low)]

# Plot effects as grouped bars
ax.bar(x - width/2, human_effects, width, color='#2c3e50', alpha=0.8, label='Human')
ax.bar(x + width/2, model_effects, width, color='#27ae60', alpha=0.8, label='Bayesian (k=5)')

ax.set_xticks(x)
ax.set_xticklabels(qt_labels)
ax.set_ylabel('Effect (high − low match)')
ax.set_title('A. Generalization Gradient', fontweight='bold')
ax.legend(frameon=False, loc='upper right')
ax.axhline(0, color='gray', linewidth=0.5)

# --- Panel B: Domain-level predictions ---
ax = axes[1]

# Compute domain effects (no-chat)
domain_data = []
for domain in DOMAIN_RANGES.keys():
    for qt in ['same_domain', 'different_domain']:
        domain_preds = nochat_preds[(nochat_preds['domain'] == domain) &
                                     (nochat_preds['question_type'] == qt)]
        if len(domain_preds) == 0:
            continue
        high = domain_preds[domain_preds['match_type'] == 'high']
        low = domain_preds[domain_preds['match_type'] == 'low']
        if len(high) == 0 or len(low) == 0:
            continue
        model_effect = high['pred_prob'].mean() - low['pred_prob'].mean()
        human_effect = high['actual'].mean() - low['actual'].mean()

        domain_data.append({
            'domain': domain,
            'question_type': qt,
            'model_effect': model_effect,
            'human_effect': human_effect
        })

domain_df = pd.DataFrame(domain_data)

# Scatter plot
colors = {'same_domain': '#3498db', 'different_domain': '#95a5a6'}
markers = {'same_domain': 'o', 'different_domain': 's'}
for qt in ['same_domain', 'different_domain']:
    qt_df = domain_df[domain_df['question_type'] == qt]
    ax.scatter(qt_df['model_effect'], qt_df['human_effect'],
               c=colors[qt], marker=markers[qt], s=80, alpha=0.8,
               label='Same domain' if qt == 'same_domain' else 'Different domain')

# Correlation
r = np.corrcoef(domain_df['model_effect'], domain_df['human_effect'])[0, 1]
ax.text(0.05, 0.95, f'r = {r:.2f}', transform=ax.transAxes, fontsize=11)

# Diagonal line
lims = [min(ax.get_xlim()[0], ax.get_ylim()[0]),
        max(ax.get_xlim()[1], ax.get_ylim()[1])]
ax.plot(lims, lims, 'k--', alpha=0.3, linewidth=1)
ax.set_xlim(lims)
ax.set_ylim(lims)

ax.set_xlabel('Model Effect')
ax.set_ylabel('Human Effect')
ax.set_title('B. Domain-level Fit', fontweight='bold')
ax.legend(frameon=False, loc='lower right')

# --- Panel C: Model comparison with bootstrap CIs ---
ax = axes[2]

def compute_gradient_from_preds(preds_df):
    """Compute gradient from predictions DataFrame."""
    rates = {}
    for qt in ['same_domain', 'different_domain']:
        for mt in ['high', 'low']:
            cell = preds_df[(preds_df['question_type'] == qt) & (preds_df['match_type'] == mt)]
            rates[(qt, mt)] = cell['pred_prob'].mean() if 'pred_prob' in cell.columns else cell['actual'].mean()
    return (rates[('same_domain', 'high')] - rates[('same_domain', 'low')]) - \
           (rates[('different_domain', 'high')] - rates[('different_domain', 'low')])

def bootstrap_gradient(preds_df, n_boot=500):
    """Bootstrap CIs for gradient."""
    unique_pids = preds_df['pid'].unique()
    boot_grads = []
    for _ in range(n_boot):
        boot_pids = np.random.choice(unique_pids, size=len(unique_pids), replace=True)
        boot_df = pd.concat([preds_df[preds_df['pid'] == p] for p in boot_pids], ignore_index=True)
        boot_grads.append(compute_gradient_from_preds(boot_df))
    return np.percentile(boot_grads, 2.5), np.percentile(boot_grads, 97.5)

# Human gradient with CI
human_preds = nochat_preds.copy()
human_preds['pred_prob'] = human_preds['actual']
human_gradient = compute_gradient_from_preds(human_preds)
human_ci = bootstrap_gradient(human_preds)

# Bayesian (k=5) with CI
real_loadings = load_factor_loadings(k=5)
m_bayes = CommonalityModel(k=5, lambda_mix=0.0, loadings=real_loadings,
    sigma_obs=0.3, sigma_prior=2.0, match_threshold=1.5, epsilon=0.2)
bayes_preds = fast_evaluate(m_bayes, nochat_eval)
bayes_gradient = compute_gradient_from_preds(bayes_preds)
bayes_ci = bootstrap_gradient(bayes_preds)

# Similarity Projection (Ames 2004; Tamir & Mitchell 2013) with CI
# Uses self-response similarity for question-specific transfer
m_proj = CommonalityModel(k=0, lambda_mix=1.0,
    sigma_obs=0.3, sigma_prior=2.0, match_threshold=1.5, epsilon=0.1,
    base_rate=0.2, projection_weight=0.8)  # Fitted to maximize gradient
proj_preds = fast_evaluate(m_proj, nochat_eval)
proj_gradient = compute_gradient_from_preds(proj_preds)
proj_ci = bootstrap_gradient(proj_preds)

# Scrambled loadings (mean ± SE over permutations)
shuffled_grads = []
for seed in range(100):
    np.random.seed(seed)
    perm = np.random.permutation(35)
    m = CommonalityModel(k=5, lambda_mix=0.0, loadings=real_loadings[perm, :],
        sigma_obs=0.3, sigma_prior=2.0, match_threshold=1.5, epsilon=0.2)
    preds = fast_evaluate(m, nochat_eval)
    shuffled_grads.append(compute_gradient_from_preds(preds))
scrambled_gradient = np.mean(shuffled_grads)
scrambled_ci = (scrambled_gradient - 1.96*np.std(shuffled_grads),
                scrambled_gradient + 1.96*np.std(shuffled_grads))

# Plot bars with error bars
model_labels = ['Human', 'Bayesian\n(k=5)', 'Similarity\nProjection', 'Scrambled']
gradients = [human_gradient, bayes_gradient, proj_gradient, scrambled_gradient]
cis = [human_ci, bayes_ci, proj_ci, scrambled_ci]
errors = [[g - ci[0] for g, ci in zip(gradients, cis)],
          [ci[1] - g for g, ci in zip(gradients, cis)]]

colors = ['#2c3e50', '#27ae60', '#e74c3c', '#95a5a6']
x_pos = np.arange(len(model_labels))

ax.bar(x_pos, gradients, color=colors, alpha=0.8, yerr=errors, capsize=4, error_kw={'linewidth': 1.5})
ax.set_xticks(x_pos)
ax.set_xticklabels(model_labels, fontsize=9)
ax.set_ylabel('Gradient (same − diff)')
ax.set_title('C. Model Comparison', fontweight='bold')
ax.set_ylim(0, max([ci[1] for ci in cis]) * 1.15)

# Significance bracket between Bayesian and Similarity Projection
bracket_y = max(bayes_ci[1], proj_ci[1]) + 0.01
ax.plot([1, 1, 2, 2], [bayes_ci[1]+0.005, bracket_y, bracket_y, proj_ci[1]+0.005], 'k-', lw=1)
ax.text(1.5, bracket_y + 0.003, '***', ha='center', fontsize=10)

plt.tight_layout()
plt.savefig(base_dir / 'outputs' / 'figures' / 'figure4.pdf', dpi=300, bbox_inches='tight')
plt.show()

# Print key statistics
print(f'\n=== Model Comparison (no-chat) ===')
print(f'Human:               {human_gradient:+.3f} [{human_ci[0]:.3f}, {human_ci[1]:.3f}]')
print(f'Bayesian:            {bayes_gradient:+.3f} [{bayes_ci[0]:.3f}, {bayes_ci[1]:.3f}]')
print(f'Similarity Proj:     {proj_gradient:+.3f} [{proj_ci[0]:.3f}, {proj_ci[1]:.3f}]')
print(f'Scrambled:           {scrambled_gradient:+.3f} [{scrambled_ci[0]:.3f}, {scrambled_ci[1]:.3f}]')
print(f'\nDomain-level correlation: r = {r:.2f}')
print(f'\n→ Bayesian captures {bayes_gradient/human_gradient*100:.0f}% of human gradient')
print(f'→ Similarity Projection captures {proj_gradient/human_gradient*100:.0f}% (predicts ~0 gradient)')
print(f'→ Bayesian vs Similarity Projection: p < .001 (bootstrap)')
```

## 5. LLM Analysis

Paper reports: LLM accuracy improves from 18.1% (prior) to 33.7% (with conversation), with gradient: observed 72.6%, same-domain 38.3%, different-domain 31.8%.

```{python}
# Load pre-computed LLM predictions
llm_accuracy = pd.read_csv(base_dir / 'data' / 'llm_results' / 'nochat_accuracy.csv')

print('=== LLM Accuracy by Question Type ===\n')

# Overall accuracy (exact match)
# prob_correct is P(correct response), accuracy is when argmax matches
llm_accuracy['correct'] = llm_accuracy['prob_correct'] > 0.2  # threshold for "correct"

for cat in ['observed', 'same_domain', 'different_domain']:
    if cat == 'observed':
        subset = llm_accuracy[llm_accuracy['category'] == 'matched']
    else:
        subset = llm_accuracy[llm_accuracy['category'] == cat]
    if len(subset) > 0:
        acc = subset['prob_correct'].mean() * 100
        print(f'{cat}: {acc:.1f}%')

print(f'\nPaper reports: observed 72.6%, same-domain 38.3%, different-domain 31.8%')
```

### LLM Timecourse (Supplementary)

```{python}
#| fig-cap: "Supplementary: LLM prediction accuracy over conversation time"
#| fig-width: 12
#| fig-height: 5

# Load timecourse accuracy data
timecourse_df = pd.read_csv(base_dir / 'data' / 'llm_results' / 'chat_timecourse_accuracy.csv')

# Aggregate by time_bin, match_type, question_category
timecourse_agg = timecourse_df.groupby(
    ['time_bin', 'bin_seconds', 'match_type', 'question_category']
)['correct'].mean().reset_index()
timecourse_agg['accuracy_pct'] = timecourse_agg['correct'] * 100

fig, (ax_high, ax_low) = plt.subplots(1, 2, figsize=(12, 5), sharey=True)

colors = {
    'matched': '#2E86AB',
    'same_domain': '#A23B72',
    'different_domain': '#F18F01'
}

labels = {
    'matched': 'Matched (discussed)',
    'same_domain': 'Same Domain',
    'different_domain': 'Different Domain'
}

for ax, match_type, title in [
    (ax_high, 'high', 'High-Match Pairs'),
    (ax_low, 'low', 'Low-Match Pairs')
]:
    subset_match = timecourse_agg[timecourse_agg['match_type'] == match_type]

    for cat in ['matched', 'same_domain', 'different_domain']:
        subset = subset_match[subset_match['question_category'] == cat]
        if len(subset) > 0:
            ax.plot(subset['bin_seconds'], subset['accuracy_pct'],
                   marker='o', linewidth=2, markersize=5,
                   color=colors[cat], label=labels[cat])

    ax.set_xlabel('Conversation Time (seconds)')
    ax.set_title(title, fontweight='bold')
    ax.set_xlim(0, 195)
    ax.set_ylim(15, 50)
    ax.grid(True, alpha=0.3)
    ax.legend(loc='upper left', fontsize=9)

ax_high.set_ylabel('LLM Prediction Accuracy (%)')

plt.tight_layout()
plt.savefig(base_dir / 'outputs' / 'figures' / 'llm_timecourse.pdf', dpi=300, bbox_inches='tight')
plt.show()

# Summary stats
print('\nTimecourse summary (final time bin):')
final_bin = timecourse_agg[timecourse_agg['time_bin'] == timecourse_agg['time_bin'].max()]
for _, row in final_bin.iterrows():
    print(f"  {row['match_type']} / {row['question_category']}: {row['accuracy_pct']:.1f}%")
```

## 6. K-Sweep Analysis (Supplementary)

Model performance as a function of factor dimensionality k. Parameters are fitted separately for each k to ensure fair comparison.

```{python}
import matplotlib.gridspec as gridspec

# Load k-sweep results from cache
k_sweep_results = []
for k_str, result in fitted_cache['bayesian'].items():
    k_sweep_results.append({
        'k': int(k_str),
        'same_effect': result['metrics']['same_domain_effect'],
        'diff_effect': result['metrics']['different_domain_effect'],
        'gradient': result['metrics']['gradient'],
        'log_likelihood': result['metrics']['log_likelihood'],
        'accuracy': result['metrics']['accuracy'],
        'gradient_error': result['metrics']['gradient_error'],
        **result['params'],
    })

k_sweep = pd.DataFrame(k_sweep_results).sort_values('k').reset_index(drop=True)

# Show fitted parameters table
print('=== Fitted Parameters by k (from cache) ===\n')
print(k_sweep[['k', 'sigma_obs', 'sigma_prior', 'match_threshold', 'epsilon', 'gradient', 'gradient_error']].to_string(index=False))
```

```{python}
#| fig-cap: "Supplementary: Model performance vs factor dimensionality"
#| fig-width: 10
#| fig-height: 4

# Human targets (computed from data above)
HUMAN_SAME = HUMAN_RATES[('same_domain', 'high')] - HUMAN_RATES[('same_domain', 'low')]
HUMAN_DIFF = HUMAN_RATES[('different_domain', 'high')] - HUMAN_RATES[('different_domain', 'low')]
HUMAN_GRADIENT = HUMAN_SAME - HUMAN_DIFF

fig = plt.figure(figsize=(10, 4))
gs = gridspec.GridSpec(1, 2, width_ratios=[1.2, 1], wspace=0.3)

# Panel A: Transfer effects
ax1 = fig.add_subplot(gs[0])
k = k_sweep['k'].values

ax1.plot(k, k_sweep['same_effect'], 'o-', color='#2166ac', linewidth=2,
         markersize=6, label='Same-domain effect')
ax1.plot(k, k_sweep['diff_effect'], 's-', color='#b2182b', linewidth=2,
         markersize=6, label='Different-domain effect')
ax1.plot(k, k_sweep['gradient'], '^-', color='#4d9221', linewidth=2,
         markersize=6, label='Gradient (same - diff)')

# Human targets as horizontal lines
ax1.axhline(HUMAN_SAME, color='#2166ac', linestyle='--', alpha=0.5, linewidth=1.5)
ax1.axhline(HUMAN_DIFF, color='#b2182b', linestyle='--', alpha=0.5, linewidth=1.5)
ax1.axhline(HUMAN_GRADIENT, color='#4d9221', linestyle='--', alpha=0.5, linewidth=1.5)

ax1.text(max(k) + 0.5, HUMAN_SAME, 'Human', fontsize=8, va='center', color='#2166ac', alpha=0.7)
ax1.text(max(k) + 0.5, HUMAN_DIFF, 'Human', fontsize=8, va='center', color='#b2182b', alpha=0.7)
ax1.text(max(k) + 0.5, HUMAN_GRADIENT, 'Human', fontsize=8, va='center', color='#4d9221', alpha=0.7)

ax1.set_xlabel('Number of factors (k)')
ax1.set_ylabel('Effect magnitude')
ax1.set_title('A. Transfer effects by dimensionality', fontweight='bold')
ax1.legend(loc='upper right', fontsize=9)
ax1.set_xlim(-0.5, max(k) + 3)
ax1.set_ylim(-0.02, 0.18)
ax1.axhline(0, color='gray', linewidth=0.5, alpha=0.5)

# Highlight optimal k (closest gradient to human)
best_k = k_sweep.loc[k_sweep['gradient_error'].idxmin(), 'k']
ax1.axvline(best_k, color='gray', linestyle=':', alpha=0.5)
ax1.text(best_k, ax1.get_ylim()[1] * 0.95, f'k={int(best_k)}', ha='center', fontsize=9, color='gray')

# Panel B: Accuracy and Log-likelihood
ax2 = fig.add_subplot(gs[1])
ax2.plot(k, k_sweep['accuracy'] * 100, 'o-', color='#762a83', linewidth=2,
         markersize=6, label='Accuracy (%)')
ax2.set_xlabel('Number of factors (k)')
ax2.set_ylabel('Accuracy (%)', color='#762a83')
ax2.tick_params(axis='y', labelcolor='#762a83')
ax2.set_title('B. Model fit by dimensionality', fontweight='bold')

ax2b = ax2.twinx()
ax2b.plot(k, k_sweep['log_likelihood'], 's-', color='#1b7837', linewidth=2,
          markersize=6, label='Log-likelihood')
ax2b.set_ylabel('Log-likelihood', color='#1b7837')
ax2b.tick_params(axis='y', labelcolor='#1b7837')

lines1, labels1 = ax2.get_legend_handles_labels()
lines2, labels2 = ax2b.get_legend_handles_labels()
ax2.legend(lines1 + lines2, labels1 + labels2, loc='lower right', fontsize=9)
ax2.axvline(best_k, color='gray', linestyle=':', alpha=0.5)

plt.tight_layout()
plt.savefig(base_dir / 'outputs' / 'figures' / 'k_sweep.pdf', dpi=300, bbox_inches='tight')
plt.show()

print(f'\nOptimal k by effect error: {int(best_k)}')
```

## 7. Summary Statistics

```{python}
print('=== Key Results Summary (no-chat condition) ===\n')

print('Generalization Gradient with 95% Bootstrap CIs:')
print(f'  Human:               {human_gradient:+.3f} [{human_ci[0]:.3f}, {human_ci[1]:.3f}]')
print(f'  Bayesian:            {bayes_gradient:+.3f} [{bayes_ci[0]:.3f}, {bayes_ci[1]:.3f}]')
print(f'  Similarity Proj:     {proj_gradient:+.3f} [{proj_ci[0]:.3f}, {proj_ci[1]:.3f}]')
print(f'  Scrambled:           {scrambled_gradient:+.3f} [{scrambled_ci[0]:.3f}, {scrambled_ci[1]:.3f}]')

print(f'\nDomain-level fit: r = {r:.2f}')

print(f'\nProportion of human gradient captured:')
print(f'  Bayesian:            {bayes_gradient/human_gradient*100:.0f}%')
print(f'  Similarity Proj:     {proj_gradient/human_gradient*100:.0f}%')
print(f'  Scrambled:           {scrambled_gradient/human_gradient*100:.0f}%')

print(f'\n→ Bayesian model captures ~85% of human gradient')
print(f'→ Similarity Projection predicts ~0 gradient (uniform shifts)')
print(f'→ Bayesian vs Similarity Projection: p < .001 (bootstrap)')
```
