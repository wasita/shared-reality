---
title: "Computational Model Analyses"
author: "Reproducibility Code"
format:
  html:
    code-fold: true
    toc: true
    toc-depth: 3
jupyter: python3
execute:
  warning: false
  message: false
---

This document reproduces the computational model analyses and Figure 4 from the paper.

1. **Setup** - Load model and data
2. **Model Comparison** - Nested model evaluation
3. **Figure 4** - Main model figure
4. **LLM Analysis** - Pre-computed LLM predictions
5. **Summary** - Key statistics

## 1. Setup

```{python}
import sys
from pathlib import Path
import numpy as np
import pandas as pd
import matplotlib.pyplot as plt

# Find project root
base_dir = Path.cwd().resolve()
while base_dir.name and not (base_dir / 'data').exists():
    base_dir = base_dir.parent
sys.path.insert(0, str(base_dir))

from models.bayesian_factor_model import (
    BayesianFactorModel,
    load_factor_loadings,
    load_evaluation_data,
    DOMAIN_RANGES,
    N_QUESTIONS,
)

print(f'Project root: {base_dir}')
```

```{python}
# --- Evaluation utilities (transparent, not hidden in module) ---

def run_evaluation(model, data):
    """Run model on all participants, return predictions DataFrame."""
    results = []
    for pid in data["pid"].unique():
        subj = data[data["pid"] == pid]
        matched = subj[subj["is_matched"] == True]
        if len(matched) == 0:
            continue

        obs_q = int(matched["matchedIdx"].iloc[0]) - 1
        r_partner = matched["partner_response"].iloc[0]
        if pd.isna(r_partner):
            continue

        r_self = np.zeros(N_QUESTIONS)
        for _, row in subj.iterrows():
            r_self[int(row["question"]) - 1] = row["preChatResponse"]

        preds = model.predict(obs_q, float(r_partner), r_self)

        for _, row in subj.iterrows():
            results.append({
                "pid": pid,
                "question": row["question"],
                "question_domain": row["preChatDomain"],
                "match_type": matched["match_type"].iloc[0],
                "question_type": row["question_type"],
                "pred_prob": preds[int(row["question"]) - 1],
                "actual": row["participant_binary_prediction"],
            })
    return pd.DataFrame(results)


def compute_metrics(pred_df, human_rates=None):
    """Compute evaluation metrics from predictions."""
    probs = pred_df["pred_prob"].values
    actual = pred_df["actual"].values

    results = {
        'log_likelihood': np.sum(actual * np.log(probs + 1e-10) + (1 - actual) * np.log(1 - probs + 1e-10)),
        'accuracy': np.mean((probs > 0.5) == actual),
    }

    # Cell-level rates and effects
    model_rates = {}
    for qt in ['observed', 'same_domain', 'different_domain']:
        for mt in ['high', 'low']:
            cell = pred_df[(pred_df["question_type"] == qt) & (pred_df["match_type"] == mt)]
            model_rates[(qt, mt)] = cell["pred_prob"].mean() if len(cell) else 0.5
        results[f'{qt}_effect'] = model_rates[(qt, 'high')] - model_rates[(qt, 'low')]

    results['model_rates'] = model_rates

    if human_rates:
        m = [model_rates[k] for k in sorted(human_rates.keys())]
        h = [human_rates[k] for k in sorted(human_rates.keys())]
        results['correlation'] = float(np.corrcoef(m, h)[0, 1])

    return results
```

```{python}
# Load behavioral data
data = load_evaluation_data()
print(f'Loaded {len(data)} observations from {data["pid"].nunique()} participants')

# Compute human rates from data (not hard-coded)
HUMAN_RATES = {}
for qt in ['observed', 'same_domain', 'different_domain']:
    for mt in ['high', 'low']:
        subset = data[(data['question_type'] == qt) & (data['match_type'] == mt)]
        HUMAN_RATES[(qt, mt)] = subset['participant_binary_prediction'].mean()

print('\nHuman rates (from data):')
for (qt, mt), rate in sorted(HUMAN_RATES.items()):
    print(f'  {qt}, {mt}: {rate:.3f}')

# Load factor structure
loadings = load_factor_loadings(k=4)
print(f'\nFactor loadings shape: {loadings.shape}')
```

## 2. Model Comparison

Paper reports nested model comparison with k ∈ {1, 4, 35} factors.

```{python}
# Model parameters (from paper)
PARAMS = {
    'sigma_obs': 0.3,
    'sigma_prior': 2.0,
    'match_threshold': 1.5,
    'epsilon': 0.4,
}

# Define nested models
model_specs = {
    'k=1 (baseline)': {'k': 1, 'infer_lambda': True},
    'k=4 (factor)': {'k': 4, 'infer_lambda': True},
    'k=35 (full cov)': {'k': 35, 'infer_lambda': True},
}
```

```{python}
#| output: false
# Run evaluation for each model
results = {}
predictions = {}

for name, spec in model_specs.items():
    print(f'Evaluating {name}...')
    model = BayesianFactorModel(
        k=spec['k'],
        infer_lambda=spec['infer_lambda'],
        **PARAMS
    )
    pred_df = run_evaluation(model, data)
    pred_df['model'] = name
    predictions[name] = pred_df
    results[name] = compute_metrics(pred_df, human_rates=HUMAN_RATES)

# Combine all predictions
all_preds = pd.concat(predictions.values(), ignore_index=True)
```

```{python}
# Print comparison table
print('=== Model Comparison ===\n')
print(f'{"Model":<20} {"Same Effect":>12} {"Diff Effect":>12} {"Gradient":>10}')
print('-' * 56)

# Human baseline
human_same = HUMAN_RATES[('same_domain', 'high')] - HUMAN_RATES[('same_domain', 'low')]
human_diff = HUMAN_RATES[('different_domain', 'high')] - HUMAN_RATES[('different_domain', 'low')]
print(f'{"Human":<20} {human_same:>+12.3f} {human_diff:>+12.3f} {human_same - human_diff:>10.3f}')

for name, r in results.items():
    same = r['same_domain_effect']
    diff = r['different_domain_effect']
    print(f'{name:<20} {same:>+12.3f} {diff:>+12.3f} {same - diff:>10.3f}')
```

## 3. Figure 4

```{python}
#| fig-cap: "Figure 4: Factor structure captures systematic generalization"
#| fig-width: 12
#| fig-height: 4

# Get main model predictions (k=4)
main_preds = predictions['k=4 (factor)'].copy()
main_preds['domain'] = main_preds['question_domain'].str.lower()

fig, axes = plt.subplots(1, 3, figsize=(12, 4))

# --- Panel A: Generalization Gradient ---
ax = axes[0]

# Color palette (consistent with behavioral figures)
LOW_COLOR = '#648FFF'   # blue
HIGH_COLOR = '#DC267F'  # pink

qt_labels = ['Observed', 'Same\nDomain', 'Different\nDomain']
qt_keys = ['observed', 'same_domain', 'different_domain']
x = np.arange(len(qt_keys))
width = 0.2

# Get human rates
human_low = [HUMAN_RATES[(qt, 'low')] for qt in qt_keys]
human_high = [HUMAN_RATES[(qt, 'high')] for qt in qt_keys]

# Get model rates
model_low, model_high = [], []
for qt in qt_keys:
    qt_preds = main_preds[main_preds['question_type'] == qt]
    model_high.append(qt_preds[qt_preds['match_type'] == 'high']['pred_prob'].mean())
    model_low.append(qt_preds[qt_preds['match_type'] == 'low']['pred_prob'].mean())

# Plot grouped bars: Human (solid), Model (hatched)
ax.bar(x - width*1.5, human_low, width, color=LOW_COLOR, alpha=0.8, label='Human')
ax.bar(x - width*0.5, human_high, width, color=HIGH_COLOR, alpha=0.8)
ax.bar(x + width*0.5, model_low, width, color=LOW_COLOR, alpha=0.8, hatch='///', edgecolor='white', label='Model')
ax.bar(x + width*1.5, model_high, width, color=HIGH_COLOR, alpha=0.8, hatch='///', edgecolor='white')

ax.set_xticks(x)
ax.set_xticklabels(qt_labels)
ax.set_ylabel('P(predict shared)')
ax.set_title('A. Generalization Gradient', fontweight='bold')
ax.legend(frameon=False, loc='upper right')

# --- Panel B: Domain-level predictions ---
ax = axes[1]

# Compute domain effects
domain_data = []
for domain in DOMAIN_RANGES.keys():
    for qt in ['same_domain', 'different_domain']:
        domain_preds = main_preds[(main_preds['domain'] == domain) &
                                   (main_preds['question_type'] == qt)]
        if len(domain_preds) == 0:
            continue
        high = domain_preds[domain_preds['match_type'] == 'high']['pred_prob'].mean()
        low = domain_preds[domain_preds['match_type'] == 'low']['pred_prob'].mean()
        model_effect = high - low

        # Get human effect (approximate from data if available)
        human_high = domain_preds[domain_preds['match_type'] == 'high']['actual'].mean()
        human_low = domain_preds[domain_preds['match_type'] == 'low']['actual'].mean()
        human_effect = human_high - human_low

        domain_data.append({
            'domain': domain,
            'question_type': qt,
            'model_effect': model_effect,
            'human_effect': human_effect
        })

domain_df = pd.DataFrame(domain_data)

# Scatter plot
colors = {'same_domain': '#3498db', 'different_domain': '#95a5a6'}
markers = {'same_domain': 'o', 'different_domain': 's'}
for qt in ['same_domain', 'different_domain']:
    qt_df = domain_df[domain_df['question_type'] == qt]
    ax.scatter(qt_df['model_effect'], qt_df['human_effect'],
               c=colors[qt], marker=markers[qt], s=80, alpha=0.8,
               label='Same domain' if qt == 'same_domain' else 'Different domain')

# Correlation
r = np.corrcoef(domain_df['model_effect'], domain_df['human_effect'])[0, 1]
ax.text(0.05, 0.95, f'r = {r:.2f}', transform=ax.transAxes, fontsize=11)

# Diagonal line
lims = [min(ax.get_xlim()[0], ax.get_ylim()[0]),
        max(ax.get_xlim()[1], ax.get_ylim()[1])]
ax.plot(lims, lims, 'k--', alpha=0.3, linewidth=1)
ax.set_xlim(lims)
ax.set_ylim(lims)

ax.set_xlabel('Model Effect')
ax.set_ylabel('Human Effect')
ax.set_title('B. Domain-level Fit', fontweight='bold')
ax.legend(frameon=False, loc='lower right')

# --- Panel C: Model comparison ---
ax = axes[2]

# Compute log-likelihoods
model_names = list(results.keys())
lls = []
for name in model_names:
    pred_df = predictions[name]
    probs = pred_df['pred_prob'].clip(1e-10, 1 - 1e-10)
    actual = pred_df['actual'].values
    ll = np.sum(actual * np.log(probs) + (1 - actual) * np.log(1 - probs))
    lls.append(ll)

# Normalize to baseline
baseline_ll = lls[0]
delta_lls = [ll - baseline_ll for ll in lls]

colors = ['#e74c3c', '#2ecc71', '#3498db']
bars = ax.bar(range(len(model_names)), delta_lls, color=colors, alpha=0.8)
ax.set_xticks(range(len(model_names)))
ax.set_xticklabels(['k=1\n(baseline)', 'k=4\n(factor)', 'k=35\n(full)'])
ax.set_ylabel('ΔLL vs baseline')
ax.set_title('C. Model Comparison', fontweight='bold')
ax.axhline(y=0, color='gray', linestyle='-', linewidth=0.5)

plt.tight_layout()
plt.savefig(base_dir / 'outputs' / 'figures' / 'figure4.pdf', dpi=300, bbox_inches='tight')
plt.show()

print(f'\nDomain-level correlation: r = {r:.2f}')
print(f'Paper reports: r = 0.91')
```

## 4. LLM Analysis

Paper reports: LLM accuracy improves from 18.1% (prior) to 33.7% (with conversation), with gradient: observed 72.6%, same-domain 38.3%, different-domain 31.8%.

```{python}
# Load pre-computed LLM predictions
llm_accuracy = pd.read_csv(base_dir / 'data' / 'llm_results' / 'nochat_accuracy.csv')

print('=== LLM Accuracy by Question Type ===\n')

# Overall accuracy (exact match)
# prob_correct is P(correct response), accuracy is when argmax matches
llm_accuracy['correct'] = llm_accuracy['prob_correct'] > 0.2  # threshold for "correct"

for cat in ['observed', 'same_domain', 'different_domain']:
    if cat == 'observed':
        subset = llm_accuracy[llm_accuracy['category'] == 'matched']
    else:
        subset = llm_accuracy[llm_accuracy['category'] == cat]
    if len(subset) > 0:
        acc = subset['prob_correct'].mean() * 100
        print(f'{cat}: {acc:.1f}%')

print(f'\nPaper reports: observed 72.6%, same-domain 38.3%, different-domain 31.8%')
```

### LLM Timecourse (Supplementary)

```{python}
#| fig-cap: "Supplementary: LLM prediction accuracy over conversation time"
#| fig-width: 12
#| fig-height: 5

# Load timecourse accuracy data
timecourse_df = pd.read_csv(base_dir / 'data' / 'llm_results' / 'chat_timecourse_accuracy.csv')

# Aggregate by time_bin, match_type, question_category
timecourse_agg = timecourse_df.groupby(
    ['time_bin', 'bin_seconds', 'match_type', 'question_category']
)['correct'].mean().reset_index()
timecourse_agg['accuracy_pct'] = timecourse_agg['correct'] * 100

fig, (ax_high, ax_low) = plt.subplots(1, 2, figsize=(12, 5), sharey=True)

colors = {
    'matched': '#2E86AB',
    'same_domain': '#A23B72',
    'different_domain': '#F18F01'
}

labels = {
    'matched': 'Matched (discussed)',
    'same_domain': 'Same Domain',
    'different_domain': 'Different Domain'
}

for ax, match_type, title in [
    (ax_high, 'high', 'High-Match Pairs'),
    (ax_low, 'low', 'Low-Match Pairs')
]:
    subset_match = timecourse_agg[timecourse_agg['match_type'] == match_type]

    for cat in ['matched', 'same_domain', 'different_domain']:
        subset = subset_match[subset_match['question_category'] == cat]
        if len(subset) > 0:
            ax.plot(subset['bin_seconds'], subset['accuracy_pct'],
                   marker='o', linewidth=2, markersize=5,
                   color=colors[cat], label=labels[cat])

    ax.set_xlabel('Conversation Time (seconds)')
    ax.set_title(title, fontweight='bold')
    ax.set_xlim(0, 195)
    ax.set_ylim(15, 50)
    ax.grid(True, alpha=0.3)
    ax.legend(loc='upper left', fontsize=9)

ax_high.set_ylabel('LLM Prediction Accuracy (%)')

plt.tight_layout()
plt.savefig(base_dir / 'outputs' / 'figures' / 'llm_timecourse.pdf', dpi=300, bbox_inches='tight')
plt.show()

# Summary stats
print('\nTimecourse summary (final time bin):')
final_bin = timecourse_agg[timecourse_agg['time_bin'] == timecourse_agg['time_bin'].max()]
for _, row in final_bin.iterrows():
    print(f"  {row['match_type']} / {row['question_category']}: {row['accuracy_pct']:.1f}%")
```

## 5. K-Sweep Analysis (Supplementary)

Model performance as a function of factor dimensionality k.

```{python}
#| fig-cap: "Supplementary: Model performance vs factor dimensionality"
#| fig-width: 10
#| fig-height: 4

import matplotlib.gridspec as gridspec

# Compute k-sweep (evaluates model at different factor dimensionalities)
k_values = [0, 1, 2, 3, 4, 5, 6, 8, 10, 15, 20, 25, 30, 35]
k_sweep_results = []

print('Computing k-sweep...')
for k_val in k_values:
    model = BayesianFactorModel(k=k_val, sigma_obs=0.3, sigma_prior=2.0,
                                  match_threshold=1.5, epsilon=0.4, infer_lambda=True)
    pred_df = run_evaluation(model, data)
    metrics = compute_metrics(pred_df, human_rates=HUMAN_RATES)

    k_sweep_results.append({
        'k': k_val,
        'same_effect': metrics['same_domain_effect'],
        'diff_effect': metrics['different_domain_effect'],
        'gradient': metrics['same_domain_effect'] - metrics['different_domain_effect'],
        'log_likelihood': metrics['log_likelihood'],
        'accuracy': metrics['accuracy'],
    })
    print(f'  k={k_val}: gradient={k_sweep_results[-1]["gradient"]:.3f}')

k_sweep = pd.DataFrame(k_sweep_results)

# Human targets (computed from data above)
HUMAN_SAME = HUMAN_RATES[('same_domain', 'high')] - HUMAN_RATES[('same_domain', 'low')]
HUMAN_DIFF = HUMAN_RATES[('different_domain', 'high')] - HUMAN_RATES[('different_domain', 'low')]
HUMAN_GRADIENT = HUMAN_SAME - HUMAN_DIFF

fig = plt.figure(figsize=(10, 4))
gs = gridspec.GridSpec(1, 2, width_ratios=[1.2, 1], wspace=0.3)

# Panel A: Transfer effects
ax1 = fig.add_subplot(gs[0])
k = k_sweep['k'].values

ax1.plot(k, k_sweep['same_effect'], 'o-', color='#2166ac', linewidth=2,
         markersize=6, label='Same-domain effect')
ax1.plot(k, k_sweep['diff_effect'], 's-', color='#b2182b', linewidth=2,
         markersize=6, label='Different-domain effect')
ax1.plot(k, k_sweep['gradient'], '^-', color='#4d9221', linewidth=2,
         markersize=6, label='Gradient (same - diff)')

# Human targets as horizontal lines
ax1.axhline(HUMAN_SAME, color='#2166ac', linestyle='--', alpha=0.5, linewidth=1.5)
ax1.axhline(HUMAN_DIFF, color='#b2182b', linestyle='--', alpha=0.5, linewidth=1.5)
ax1.axhline(HUMAN_GRADIENT, color='#4d9221', linestyle='--', alpha=0.5, linewidth=1.5)

ax1.text(max(k) + 0.5, HUMAN_SAME, 'Human', fontsize=8, va='center', color='#2166ac', alpha=0.7)
ax1.text(max(k) + 0.5, HUMAN_DIFF, 'Human', fontsize=8, va='center', color='#b2182b', alpha=0.7)
ax1.text(max(k) + 0.5, HUMAN_GRADIENT, 'Human', fontsize=8, va='center', color='#4d9221', alpha=0.7)

ax1.set_xlabel('Number of factors (k)')
ax1.set_ylabel('Effect magnitude')
ax1.set_title('A. Transfer effects by dimensionality', fontweight='bold')
ax1.legend(loc='upper right', fontsize=9)
ax1.set_xlim(-0.5, max(k) + 3)
ax1.set_ylim(-0.02, 0.18)
ax1.axhline(0, color='gray', linewidth=0.5, alpha=0.5)

# Highlight optimal k (closest gradient to human)
k_sweep['gradient_error'] = abs(k_sweep['gradient'] - HUMAN_GRADIENT)
best_k = k_sweep.loc[k_sweep['gradient_error'].idxmin(), 'k']
ax1.axvline(best_k, color='gray', linestyle=':', alpha=0.5)
ax1.text(best_k, ax1.get_ylim()[1] * 0.95, f'k={int(best_k)}', ha='center', fontsize=9, color='gray')

# Panel B: Accuracy and Log-likelihood
ax2 = fig.add_subplot(gs[1])
ax2.plot(k, k_sweep['accuracy'] * 100, 'o-', color='#762a83', linewidth=2,
         markersize=6, label='Accuracy (%)')
ax2.set_xlabel('Number of factors (k)')
ax2.set_ylabel('Accuracy (%)', color='#762a83')
ax2.tick_params(axis='y', labelcolor='#762a83')
ax2.set_title('B. Model fit by dimensionality', fontweight='bold')

ax2b = ax2.twinx()
ax2b.plot(k, k_sweep['log_likelihood'], 's-', color='#1b7837', linewidth=2,
          markersize=6, label='Log-likelihood')
ax2b.set_ylabel('Log-likelihood', color='#1b7837')
ax2b.tick_params(axis='y', labelcolor='#1b7837')

lines1, labels1 = ax2.get_legend_handles_labels()
lines2, labels2 = ax2b.get_legend_handles_labels()
ax2.legend(lines1 + lines2, labels1 + labels2, loc='lower right', fontsize=9)
ax2.axvline(best_k, color='gray', linestyle=':', alpha=0.5)

plt.tight_layout()
plt.savefig(base_dir / 'outputs' / 'figures' / 'k_sweep.pdf', dpi=300, bbox_inches='tight')
plt.show()

print(f'\nOptimal k by effect error: {int(best_k)}')
```

## 6. Summary Statistics

```{python}
print('=== Key Results Summary ===\n')

# Gradient (effect = high - low)
print('Generalization Gradient (k=4 model):')
for i, qt in enumerate(qt_keys):
    effect = model_high[i] - model_low[i]
    print(f'  {qt_labels[i]}: {effect:+.3f}')

print(f'\nDomain-level fit: r = {r:.2f}')

# Model comparison
print(f'\nModel comparison (ΔLL vs k=1 baseline):')
for name, dll in zip(model_names, delta_lls):
    print(f'  {name}: {dll:+.0f}')
```
