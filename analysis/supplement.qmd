---
title: "Supplementary Information"
author: "Reproducibility Code"
format:
  html:
    code-fold: true
    toc: true
    toc-depth: 3
  pdf:
    documentclass: article
    number-sections: false
engine: knitr
execute:
  warning: false
  message: false
  cache: true
knitr:
  opts_chunk:
    message: false
    warning: false
    cache: true
---

```{r}
#| label: setup
#| include: false
Sys.setenv(RETICULATE_PYTHON = "/Users/rxdh/miniconda3/bin/python3")
library(reticulate)
use_python("/Users/rxdh/miniconda3/bin/python3", required = TRUE)
library(lme4)
library(lmerTest)
library(dplyr)
library(tidyr)
library(purrr)
```

```{python}
#| label: python-setup
#| include: false
import sys
from pathlib import Path
import pandas as pd
import numpy as np
import seaborn as sns
import matplotlib.pyplot as plt
from scipy.stats import pearsonr, linregress
from scipy import stats
from statsmodels.stats.multitest import multipletests
import polars as pl
from polars import col, lit

plt.rcParams['font.family'] = 'Helvetica Neue'
sns.set_style('whitegrid')

# Find project root
base_dir = Path.cwd().resolve()
while base_dir.name and not (base_dir / 'data').exists():
    base_dir = base_dir.parent
if not (base_dir / 'data').exists():
    raise ValueError('Could not find project root with data directory')

data_dir = base_dir / 'data'
figures_dir = base_dir / 'outputs' / 'figures'
figures_dir.mkdir(parents=True, exist_ok=True)

sys.path.insert(0, str(base_dir))

lowhigh_palette = ['#648FFF', '#DC267F']  # low (blue), high (pink)
```

```{python}
#| label: load-data
#| include: false

# Load pre-merged behavioral data
df = pl.read_csv(data_dir / 'responses.csv')
df = df.filter(~col('matchedTolerance').is_null())

# Derived variables
df = df.with_columns([
    pl.when(col('matchedTolerance') <= 1).then(lit('shared')).otherwise(lit('opposing')).alias('stance'),
    pl.when(col('matchedQuestion') == col('preChatQuestion')).then(lit('sameQ'))
     .when(col('matchedDomain') == col('preChatDomain')).then(lit('sameD'))
     .otherwise(lit('diffD')).alias('category'),
    (col('postChatResponse') - col('preChatResponse')).abs().alias('distance')
]).rename({'groupId': 'dyadId'})

# Convert to pandas with numeric contrasts
df_pd = df.to_pandas()
df_pd['stance_num'] = df_pd['stance'].map({'opposing': -1, 'shared': 1})
df_pd['category_num'] = df_pd['category'].map({'sameQ': 1, 'sameD': 0, 'diffD': -1}).astype(float)
df_pd['experiment_num'] = df_pd['experiment'].map({'no-chat': -1, 'chat': 1})
df_pd['distance_num'] = df_pd['distance'].map({0: 2, 1: 1, 2: 0, 3: -1, 4: -2})

# Save processed data for later chunks (Python state can be lost)
df_pd.to_csv('/tmp/df_pd_processed.csv', index=False)

# Inference subset (excludes observed question)
inf_df = df_pd[df_pd['question'] != df_pd['matchedIdx']]
```

## Internal Consistency of Commonality Judgments {#sec-distance}

To verify that participants' binary commonality judgments aligned with their continuous predictions about their partner's beliefs, we calculated the distance between each participant's own (pre-interaction) response and their (post-interaction) prediction of their partner's response for every survey question. We reasoned that participants should expect commonality when they predicted their partner held similar views to their own, and expect less commonality when they predicted divergent views.

A logistic mixed-effects model predicted item-level commonality expectations from this predicted distance, interaction type, stance condition, and all interactions, with random intercepts for participants. As expected, greater distance between one's own response and predicted partner response significantly reduced the likelihood of expecting commonality ($\beta = -1.32$, 95% CI $[-1.36, -1.29]$, $p < 0.001$; @fig-distance-commonality). Participants were most likely to expect commonality when they predicted their partner held identical views (distance $= 0$), with expectations declining monotonically as predicted distance increased.

Interaction type moderated this relationship ($\beta = -0.32$, 95% CI $[-0.36, -0.29]$, $p < 0.001$): participants in real conversations maintained higher commonality expectations even when predicting substantial disagreement, whereas those in imagined interactions required closer predicted alignment before expecting shared views. This pattern held across all seven topic domains, suggesting that conversation relaxed thresholds for inferring commonality.

```{python}
#| label: fig-distance-commonality
#| fig-cap: "Commonality expectations as a function of predicted distance between partner and self responses. Participants expected less commonality as the predicted distance increased. Chat participants (right) maintained higher commonality expectations than observation-only participants (left) across all distance levels. Fill color indicates stance condition (pink = shared, blue = opposing). Error bars are 95% bootstrapped confidence intervals."
#| fig-width: 10
#| fig-height: 5

# Aggregate by participant first
inf_grouped = inf_df.groupby(['experiment', 'stance', 'pid', 'distance'])['predictShared'].mean().reset_index()

# Bootstrap CI function
def bootstrap_ci(data, n_boot=1000, ci=95):
    boot_means = [np.random.choice(data, size=len(data), replace=True).mean() for _ in range(n_boot)]
    lower = np.percentile(boot_means, (100 - ci) / 2)
    upper = np.percentile(boot_means, 100 - (100 - ci) / 2)
    return lower, upper

# Compute means and CIs
summary_data = []
for exp in ['no-chat', 'chat']:
    for stance in ['opposing', 'shared']:
        for dist in range(5):
            subset = inf_grouped[(inf_grouped['experiment'] == exp) &
                                  (inf_grouped['stance'] == stance) &
                                  (inf_grouped['distance'] == dist)]['predictShared']
            if len(subset) > 0:
                mean_val = subset.mean()
                ci_low, ci_high = bootstrap_ci(subset.values)
                summary_data.append({
                    'experiment': exp, 'stance': stance, 'distance': dist,
                    'mean': mean_val, 'ci_low': ci_low, 'ci_high': ci_high
                })

summary_df = pd.DataFrame(summary_data)

# Create figure
fig, axes = plt.subplots(1, 2, figsize=(10, 5), sharey=True)
plt.rcParams.update({'font.size': 12})

exp_labels = {'no-chat': 'Imagined', 'chat': 'Real'}
stance_colors = {'opposing': '#648FFF', 'shared': '#DC267F'}  # blue, pink

for i, exp in enumerate(['no-chat', 'chat']):
    ax = axes[i]
    exp_data = summary_df[summary_df['experiment'] == exp]

    bar_width = 0.35
    x = np.arange(5)

    for j, stance in enumerate(['opposing', 'shared']):
        stance_data = exp_data[exp_data['stance'] == stance].sort_values('distance')
        offset = -bar_width/2 if stance == 'opposing' else bar_width/2

        bars = ax.bar(x + offset, stance_data['mean'], bar_width,
                      color=stance_colors[stance], alpha=0.8,
                      label=stance.capitalize() if i == 1 else '')

        # Error bars - no caps
        ax.errorbar(x + offset, stance_data['mean'],
                    yerr=[stance_data['mean'] - stance_data['ci_low'],
                          stance_data['ci_high'] - stance_data['mean']],
                    fmt='none', color='black', capsize=0, linewidth=1.5)

    ax.set_xlabel('Predicted Distance\n(|Prediction for Partner − Own|)', fontsize=14)
    ax.set_xticks(x)
    ax.set_xticklabels(['0', '1', '2', '3', '4'], fontsize=12)
    ax.set_ylim([0, 1])
    ax.set_yticks([0, 0.2, 0.4, 0.6, 0.8, 1.0])
    ax.text(-0.1, 1.05, chr(65 + i), transform=ax.transAxes, fontsize=16, fontweight='bold', va='bottom')
    ax.set_title(exp_labels[exp], fontsize=14, fontweight='bold')
    ax.tick_params(labelsize=11)
    ax.grid(False)
    ax.spines['top'].set_visible(False)
    ax.spines['right'].set_visible(False)

    if i == 0:
        ax.set_ylabel('Commonality Expectation', fontsize=14)
    if i == 1:
        ax.legend(title='Stance', frameon=False, loc='upper right', fontsize=11)

plt.tight_layout()
plt.savefig(figures_dir / 'figure_s1.pdf', dpi=300, bbox_inches='tight')
plt.show()
```

```{python}
#| output: false
inf_df.to_csv('/tmp/inf_df.csv', index=False)
```

```{r}
#| label: distance-model
inf_df <- read.csv('/tmp/inf_df.csv')
model <- glmer(predictShared ~ distance_num * experiment_num * stance_num + (1|pid),
               data = inf_df, family = binomial)
summary(model)
confint(model, method = "Wald")
```

## Factor Structure of Belief Space {#sec-factor}

We computed factor loadings via eigendecomposition of the 35-question belief correlation matrix. @fig-factor-structure panel A shows the scree plot comparing observed eigenvalues against the 95th percentile from parallel analysis (eigenvalues computed from random data with the same dimensions). The first five components exceeded the parallel analysis threshold, suggesting $k = 5$ factors. These five factors explained 36% of total variance.

@fig-factor-structure panel B displays the absolute loadings for the first five factors, organized by experimenter-assigned domain. Religion and politics questions loaded strongly on single factors (PC1 and PC3, respectively), indicating tight clustering in belief space. Lifestyle and background questions showed more diffuse loadings across multiple factors. This variation in factor structure predicts domain-specific generalization patterns: domains with coherent loadings should support stronger within-domain transfer.

```{python}
#| label: fig-factor-structure
#| fig-cap: "Factor structure of belief space. (A) Scree plot comparing observed eigenvalues (solid) against the 95th percentile from parallel analysis (dashed). The first five components exceed the threshold. (B) Absolute factor loadings for the first five principal components, grouped by domain. Religion and politics questions load strongly on single factors, while lifestyle and background questions show more diffuse loadings across factors."
#| fig-width: 12
#| fig-height: 5

# Re-import after R chunks
import sys
import pandas as pd
import numpy as np
import matplotlib.pyplot as plt
import seaborn as sns
from pathlib import Path

base_dir = Path('/Users/rxdh/Repos/shared-reality')
data_dir = base_dir / 'data'
figures_dir = base_dir / 'outputs' / 'figures'
sys.path.insert(0, str(base_dir))

from models.model import load_factor_loadings

# Load full loadings to compute eigenvalues for scree plot
loadings_full = load_factor_loadings(k=35)
eigenvalues = np.sum(loadings_full ** 2, axis=0)  # Variance per factor

# Load k=5 for heatmap
loadings_k5 = load_factor_loadings(k=5)
loadings_k5[:, 2] *= -1  # Flip factor 3 sign for interpretability
var_explained_k5 = np.sum(eigenvalues[:5]) / np.sum(eigenvalues) * 100

# Reorder domains for interpretability
domain_order = ['Religion', 'Politics', 'Background', 'Morality', 'Lifestyle', 'Identity', 'Preferences']
original_ranges = {
    'Lifestyle': (0, 5), 'Background': (5, 10), 'Identity': (10, 15),
    'Morality': (15, 20), 'Politics': (20, 25), 'Preferences': (25, 30), 'Religion': (30, 35)
}

# Build reordering index
reorder_idx = []
for domain in domain_order:
    start, end = original_ranges[domain]
    domain_qs = list(range(start, end))
    if domain == 'Preferences':
        domain_qs.sort(key=lambda q: -loadings_k5[q, 4])
    else:
        domain_qs.sort(key=lambda q: (np.argmax(np.abs(loadings_k5[q])), -np.max(np.abs(loadings_k5[q]))))
    reorder_idx.extend(domain_qs)

loadings_reord = loadings_k5[reorder_idx, :]

# Load questions
questions_df = pd.read_csv(data_dir / 'questions.csv')
questions_df['domain'] = questions_df['domain'].replace('arbitrary', 'Lifestyle').str.capitalize()
reord_labels = [questions_df.iloc[i]['text'][:40] + '...' if len(questions_df.iloc[i]['text']) > 40
                else questions_df.iloc[i]['text'] for i in reorder_idx]

# Parallel analysis
n_parallel = 1000
responses_df = pd.read_csv(data_dir / 'responses.csv', low_memory=False)
n_obs = responses_df['pid'].nunique()
parallel_eigs = np.zeros((n_parallel, 35))
for i in range(n_parallel):
    random_data = np.random.randn(n_obs, 35)
    cov = np.corrcoef(random_data.T)
    parallel_eigs[i] = np.linalg.eigvalsh(cov)[::-1]
parallel_95 = np.percentile(parallel_eigs, 95, axis=0)

# Create figure
fig, (ax1, ax2) = plt.subplots(1, 2, figsize=(12, 5), gridspec_kw={'width_ratios': [1, 1.5]})

# Panel A: Scree plot
ax1.plot(range(1, 36), eigenvalues, 'o-', color='#2166ac', linewidth=2, markersize=6, label='Observed')
ax1.plot(range(1, 36), parallel_95, 's--', color='#b2182b', linewidth=1.5, markersize=4, label='95th percentile\n(parallel analysis)')
ax1.axvline(5.5, color='gray', linestyle=':', alpha=0.6)
ax1.set_xlabel('Component', fontsize=12)
ax1.set_ylabel('Eigenvalue', fontsize=12)
ax1.set_xlim(0, 20)
ax1.legend(frameon=False, fontsize=10)
ax1.text(-0.15, 1.05, 'A', transform=ax1.transAxes, fontsize=16, fontweight='bold')
ax1.text(6, eigenvalues[4] + 0.3, 'k=5', fontsize=11, color='gray')

# Panel B: Heatmap
im = ax2.imshow(np.abs(loadings_reord), cmap='Reds', aspect='auto', vmin=0, vmax=0.8)

# Domain boundaries
domain_boundaries = []
cumsum = 0
for domain in domain_order:
    start, end = original_ranges[domain]
    domain_boundaries.append(cumsum + (end - start) / 2 - 0.5)
    cumsum += end - start

for i, (domain, ypos) in enumerate(zip(domain_order, domain_boundaries)):
    ax2.text(-0.5, ypos, domain, ha='right', va='center', fontsize=10, fontweight='bold')

# Add horizontal lines between domains
cumsum = 0
for domain in domain_order[:-1]:
    start, end = original_ranges[domain]
    cumsum += end - start
    ax2.axhline(cumsum - 0.5, color='white', linewidth=2)

ax2.set_xticks(range(5))
ax2.set_xticklabels([f'F{i+1}' for i in range(5)], fontsize=11)
ax2.set_yticks([])
ax2.set_xlabel('Factor', fontsize=12)
ax2.text(-0.05, 1.05, 'B', transform=ax2.transAxes, fontsize=16, fontweight='bold')

cbar = plt.colorbar(im, ax=ax2, shrink=0.6, label='|Loading|')

plt.tight_layout()
plt.savefig(figures_dir / 'figure_s2.pdf', dpi=300, bbox_inches='tight')
plt.show()

print(f'Variance explained by k=5 factors: {var_explained_k5:.1f}%')
```

## Parameter Sensitivity Analyses {#sec-sensitivity}

### Factor dimensionality ($k$)

We evaluated model performance across factor dimensionalities from $k = 1$ (first principal component) to $k = 35$ (full covariance matrix). @fig-k-sweep shows that performance peaked at low dimensionality and degraded at high $k$. This pattern confirms that structured compression (low $k$) captures transferable commonalities better than full covariance. High-$k$ models overfit to noise and idiosyncratic correlations, diluting the domain-specific transfer signal. The optimal $k$ from behavioral fit closely matches parallel analysis, providing independent validation of the factor structure.

```{python}
#| label: fig-k-sweep
#| fig-cap: "Model performance varies with factor dimensionality. Transfer effects as a function of k: same-domain (blue), different-domain (red), and gradient (green). Dashed lines show human targets."
#| fig-width: 7
#| fig-height: 5

from models.model import CommonalityModel, load_evaluation_data

# Load evaluation data
data = load_evaluation_data()
nochat_data = data[data['experiment'] == 'no-chat']

# Compute human rates for no-chat condition
NOCHAT_HUMAN = {}
for qt in ['same_domain', 'different_domain']:
    for mt in ['high', 'low']:
        cell = nochat_data[(nochat_data['question_type'] == qt) & (nochat_data['match_type'] == mt)]
        NOCHAT_HUMAN[(qt, mt)] = cell['participant_binary_prediction'].mean()

# Load fitted parameters (same as model_analyses.qmd)
import json
params_path = base_dir / 'models' / 'fitted_params.json'
with open(params_path) as f:
    fitted_cache = json.load(f)
UNIFIED_PARAMS = {**fitted_cache['unified_params'], 'lambda_mix': 0.0}

# Prepare evaluation data
nochat_eval = nochat_data[['pid', 'question', 'question_type', 'match_type',
                            'preChatResponse', 'partner_response', 'matchedIdx',
                            'participant_binary_prediction', 'is_matched', 'experiment']].copy()

def fast_evaluate(model, eval_data):
    """Fast evaluation on prepared data."""
    results = []
    for pid in eval_data['pid'].unique():
        subj = eval_data[eval_data['pid'] == pid]
        matched = subj[subj['is_matched'] == True]
        if len(matched) == 0:
            continue
        obs_q = int(matched['matchedIdx'].iloc[0]) - 1
        r_partner = matched['partner_response'].iloc[0]
        if pd.isna(r_partner):
            continue
        r_self = np.zeros(35)
        for _, row in subj.iterrows():
            r_self[int(row['question']) - 1] = row['preChatResponse']
        preds = model.predict(obs_q, float(r_partner), r_self)
        for _, row in subj.iterrows():
            results.append({
                'pid': pid,
                'question_type': row['question_type'],
                'match_type': row['match_type'],
                'pred_prob': preds[int(row['question']) - 1],
                'actual': row['participant_binary_prediction'],
            })
    return pd.DataFrame(results)

# K values to sweep
k_values = [1, 2, 3, 4, 5, 6, 7, 8, 10, 15, 20, 25, 30, 35]

# Compute metrics for each k
k_sweep_results = []
for k in k_values:
    model = CommonalityModel(k=k, **UNIFIED_PARAMS)
    preds = fast_evaluate(model, nochat_eval)

    # Effects by question type and match type
    same_high = preds[(preds['question_type'] == 'same_domain') & (preds['match_type'] == 'high')]['pred_prob'].mean()
    same_low = preds[(preds['question_type'] == 'same_domain') & (preds['match_type'] == 'low')]['pred_prob'].mean()
    diff_high = preds[(preds['question_type'] == 'different_domain') & (preds['match_type'] == 'high')]['pred_prob'].mean()
    diff_low = preds[(preds['question_type'] == 'different_domain') & (preds['match_type'] == 'low')]['pred_prob'].mean()

    same_effect = same_high - same_low
    diff_effect = diff_high - diff_low
    gradient = same_effect - diff_effect

    k_sweep_results.append({
        'k': k,
        'same_effect': same_effect,
        'diff_effect': diff_effect,
        'gradient': gradient,
    })

k_sweep = pd.DataFrame(k_sweep_results)

# Compute human targets
HUMAN_SAME_NC = NOCHAT_HUMAN[('same_domain', 'high')] - NOCHAT_HUMAN[('same_domain', 'low')]
HUMAN_DIFF_NC = NOCHAT_HUMAN[('different_domain', 'high')] - NOCHAT_HUMAN[('different_domain', 'low')]
HUMAN_GRADIENT_NC = HUMAN_SAME_NC - HUMAN_DIFF_NC

k_sweep['gradient_error'] = np.abs(k_sweep['gradient'] - HUMAN_GRADIENT_NC)

# Plot
plt.rcParams.update({
    'font.size': 14,
    'axes.labelsize': 16,
    'xtick.labelsize': 13,
    'ytick.labelsize': 13,
    'legend.fontsize': 12,
})

fig, ax = plt.subplots(figsize=(7, 5))
k = k_sweep['k'].values

ax.plot(k, k_sweep['same_effect'], 'o-', color='#2166ac', linewidth=2.5,
        markersize=8, label='Same-domain effect')
ax.plot(k, k_sweep['diff_effect'], 's-', color='#b2182b', linewidth=2.5,
        markersize=8, label='Different-domain effect')
ax.plot(k, k_sweep['gradient'], '^-', color='#4d9221', linewidth=2.5,
        markersize=8, label='Gradient (same - diff)')

# Human targets as dashed lines
ax.hlines(HUMAN_SAME_NC, 0, max(k), color='#2166ac', linestyle='--', alpha=0.6, linewidth=1.5)
ax.hlines(HUMAN_DIFF_NC, 0, max(k), color='#b2182b', linestyle='--', alpha=0.6, linewidth=1.5)
ax.hlines(HUMAN_GRADIENT_NC, 0, max(k), color='#4d9221', linestyle='--', alpha=0.6, linewidth=1.5)

ax.set_xlabel('Number of factors (k)')
ax.set_ylabel('Effect magnitude')
ax.legend(loc='upper right', frameon=False)
ax.set_xlim(0, max(k) + 1)
ax.axhline(0, color='gray', linewidth=0.5, alpha=0.5)

# Highlight optimal k
best_k = k_sweep.loc[k_sweep['gradient_error'].idxmin(), 'k']
ax.axvline(best_k, color='gray', linestyle=':', alpha=0.6, linewidth=1.5)
ax.text(best_k + 0.5, 0.01, f'k={int(best_k)}', fontsize=12, color='gray')

plt.tight_layout()
plt.savefig(figures_dir / 'figure_s3.pdf', dpi=300, bbox_inches='tight')
plt.show()

# Summary statistics
print(f'Human gradient: {HUMAN_GRADIENT_NC:.4f}')
print(f'Optimal k: {int(best_k)}')
for k_val in [1, 5, 35]:
    if k_val in k_sweep['k'].values:
        row = k_sweep[k_sweep['k'] == k_val].iloc[0]
        pct = row['gradient'] / HUMAN_GRADIENT_NC * 100
        print(f"  k={k_val}: Gradient = +{row['gradient']:.3f} ({pct:.0f}% of human)")
```

### Bayesian vs. egocentric mixture weight ($\lambda$)

To assess whether people blend Bayesian and egocentric strategies, we fit a mixture weight $\lambda \in [0, 1]$ that interpolates between the pure Bayesian model ($\lambda = 0$) and pure egocentric model ($\lambda = 1$):

$$P(\text{match}) = (1 - \lambda) \cdot P_{\text{Bayesian}}(\text{match}) + \lambda \cdot P_{\text{egocentric}}(\text{match})$$

Performance degraded monotonically as $\lambda$ increased. The pure Bayesian model substantially outperformed all alternatives. There was no evidence that adding egocentric projection improved upon pure Bayesian inference.

```{python}
#| label: lambda-sweep

# Lambda sweep
lambda_values = [0.0, 0.25, 0.5, 0.75, 1.0]
lambda_results = []

for lam in lambda_values:
    model = CommonalityModel(k=5, lambda_mix=lam, **{k: v for k, v in UNIFIED_PARAMS.items() if k != 'lambda_mix'})
    preds = fast_evaluate(model, nochat_eval)

    same_high = preds[(preds['question_type'] == 'same_domain') & (preds['match_type'] == 'high')]['pred_prob'].mean()
    same_low = preds[(preds['question_type'] == 'same_domain') & (preds['match_type'] == 'low')]['pred_prob'].mean()
    diff_high = preds[(preds['question_type'] == 'different_domain') & (preds['match_type'] == 'high')]['pred_prob'].mean()
    diff_low = preds[(preds['question_type'] == 'different_domain') & (preds['match_type'] == 'low')]['pred_prob'].mean()

    gradient = (same_high - same_low) - (diff_high - diff_low)
    lambda_results.append({'lambda': lam, 'gradient': gradient})

lambda_df = pd.DataFrame(lambda_results)

print('Lambda sweep results:')
for _, row in lambda_df.iterrows():
    pct = row['gradient'] / HUMAN_GRADIENT_NC * 100
    label = 'pure Bayesian' if row['lambda'] == 0 else ('pure egocentric' if row['lambda'] == 1 else 'mixture')
    print(f"  lambda={row['lambda']:.2f} ({label}): Gradient = +{row['gradient']:.3f} ({pct:.0f}% of human)")
```

## Diagnostic Test: Self-Structure vs. Population Structure {#sec-self-structure}

A potential concern is that if individual belief structures mirror population structure (due to shared cultural learning), then egocentric projection could produce gradients indistinguishable from Bayesian inference. To address this, we identified "diagnostic" participants whose own belief structure deviated from the population pattern and tested whether their generalization behavior followed their own structure or the population structure.

For each participant, we computed a "self-structure gradient"—the difference between within-domain and cross-domain response similarity in their own pre-chat responses. Participants in the bottom quartile (N = 126) showed weak or reversed domain clustering (mean self-structure gradient = −0.024), meaning their own responses did not cluster by domain the way the population does.

```{python}
#| label: fig-self-structure
#| fig-cap: "Diagnostic test of self-structure vs. population structure. (A) Distribution of self-structure gradients across participants. Shaded region indicates the bottom quartile (anomalous structure). (B) Behavioral gradients for participants with normal vs. anomalous self-structure. Despite having different self-structure, both groups show similar positive behavioral gradients, supporting population-based inference. Error bars are 95% bootstrap CIs."
#| fig-width: 9
#| fig-height: 4

import numpy as np
import pandas as pd
import matplotlib.pyplot as plt
from pathlib import Path

base_dir = Path('/Users/rxdh/Repos/shared-reality')
data_dir = base_dir / 'data'
figures_dir = base_dir / 'outputs' / 'figures'

import sys
sys.path.insert(0, str(base_dir))
from analysis.utils import (
    compute_gradient,
    bootstrap_gradient,
    compute_all_self_structure_gradients,
    COLORS,
)

# Load responses and compute self-structure gradients
responses_df = pd.read_csv(data_dir / 'responses.csv', low_memory=False)
response_matrix = responses_df.pivot_table(
    index='pid', columns='question', values='preChatResponse', aggfunc='first'
)
structure_gradients = compute_all_self_structure_gradients(response_matrix)

all_grads = list(structure_gradients.values())
q25 = np.percentile(all_grads, 25)

# Classify participants
nochat_data = responses_df[responses_df['experiment'] == 'no-chat'].copy()
nochat_data['self_structure_gradient'] = nochat_data['pid'].map(structure_gradients)
nochat_data['is_anomaly'] = nochat_data['self_structure_gradient'] < q25

n_normal = nochat_data[~nochat_data['is_anomaly']]['pid'].nunique()
n_anomaly = nochat_data[nochat_data['is_anomaly']]['pid'].nunique()

# Compute behavioral gradients
normal_data = nochat_data[~nochat_data['is_anomaly']]
anomaly_data = nochat_data[nochat_data['is_anomaly']]

normal_gradient = compute_gradient(normal_data, 'participant_binary_prediction')
anomaly_gradient = compute_gradient(anomaly_data, 'participant_binary_prediction')

# Bootstrap CIs
normal_boots = bootstrap_gradient(normal_data, 'participant_binary_prediction')
anomaly_boots = bootstrap_gradient(anomaly_data, 'participant_binary_prediction')

# Create figure
from analysis.utils import set_plot_style
set_plot_style()

fig, axes = plt.subplots(1, 2, figsize=(9, 4))

# Panel A: Distribution
ax = axes[0]
ax.hist(all_grads, bins=25, color=COLORS['bayesian'], alpha=0.8, edgecolor='white', linewidth=0.5)
ax.axvline(q25, color=COLORS['high'], linestyle='--', linewidth=2,
           label=f'Bottom quartile\n(N={n_anomaly})')
ax.axvline(0, color='gray', linestyle=':', linewidth=1, alpha=0.7)
ax.axvspan(min(all_grads) - 0.01, q25, alpha=0.15, color=COLORS['high'])
ax.set_xlabel('Self-structure gradient\n(within − cross domain similarity)', fontsize=11)
ax.set_ylabel('Number of participants', fontsize=11)
ax.set_title('A', fontweight='bold', loc='left', fontsize=14)
ax.legend(frameon=False, fontsize=9, loc='upper right')

# Panel B: Behavioral gradients
ax = axes[1]
width = 0.6
normal_err = [[normal_gradient - np.percentile(normal_boots, 2.5)],
              [np.percentile(normal_boots, 97.5) - normal_gradient]]
anomaly_err = [[anomaly_gradient - np.percentile(anomaly_boots, 2.5)],
               [np.percentile(anomaly_boots, 97.5) - anomaly_gradient]]

ax.bar([0], [normal_gradient], width=width, color=COLORS['bayesian'],
       yerr=normal_err, capsize=4, error_kw={'linewidth': 1.5})
ax.bar([1], [anomaly_gradient], width=width, color=COLORS['high'],
       yerr=anomaly_err, capsize=4, error_kw={'linewidth': 1.5})
ax.axhline(0, color='gray', linestyle='--', linewidth=1, alpha=0.7)

# n.s. bracket
bracket_y = max(normal_gradient + normal_err[1][0],
                anomaly_gradient + anomaly_err[1][0]) + 0.015
ax.plot([0, 0, 1, 1], [normal_gradient + normal_err[1][0] + 0.005, bracket_y,
                       bracket_y, anomaly_gradient + anomaly_err[1][0] + 0.005],
        'k-', lw=1, solid_capstyle='butt')
ax.text(0.5, bracket_y + 0.008, 'n.s.', ha='center', fontsize=10, fontstyle='italic')

ax.set_xticks([0, 1])
ax.set_xticklabels(['Normal\nstructure', 'Anomalous\nstructure'], fontsize=10)
ax.set_ylabel('Behavioral gradient\n(same − different domain)', fontsize=11)
ax.set_title('B', fontweight='bold', loc='left', fontsize=14)
ax.set_ylim(-0.02, bracket_y + 0.04)

plt.tight_layout()
plt.savefig(figures_dir / 'figure_self_structure.pdf', dpi=300, bbox_inches='tight')
plt.show()

# Print statistics
normal_self = normal_data.groupby('pid')['self_structure_gradient'].first().mean()
anomaly_self = anomaly_data.groupby('pid')['self_structure_gradient'].first().mean()

# Bootstrap p-value
diff_boots = normal_boots[:min(len(normal_boots), len(anomaly_boots))] - \
             anomaly_boots[:min(len(normal_boots), len(anomaly_boots))]
p_diff = 2 * min(np.mean(diff_boots < 0), np.mean(diff_boots > 0))

print(f"Normal structure (N={n_normal}): self-structure = {normal_self:+.3f}, behavioral gradient = {normal_gradient:+.3f}")
print(f"Anomalous structure (N={n_anomaly}): self-structure = {anomaly_self:+.3f}, behavioral gradient = {anomaly_gradient:+.3f}")
print(f"Difference in behavioral gradients: {normal_gradient - anomaly_gradient:+.3f}, p = {p_diff:.3f}")
print(f"Anomalous 95% CI: [{np.percentile(anomaly_boots, 2.5):+.3f}, {np.percentile(anomaly_boots, 97.5):+.3f}]")
```

The critical finding is that participants with anomalous self-structure (negative self-structure gradient, meaning their responses do not cluster by domain) nonetheless showed robust positive behavioral gradients (+0.123, 95% CI [0.050, 0.193]) indistinguishable from participants with typical structure (p = 0.97). If people projected from their own beliefs, these participants should show weak or reversed generalization gradients matching their own structure. Instead, all participants showed domain-structured generalization consistent with population-level knowledge.

Furthermore, individual self-structure gradient did not correlate with behavioral gradient (r = 0.023, p = 0.605), nor with same-domain P(shared) judgments (r = −0.011, p = 0.802). These null correlations confirm that generalization behavior does not track the idiosyncratic structure of one's own beliefs.

## Scrambled Loadings Control {#sec-scrambled}

To verify that model performance depends on veridical belief structure, we compared the actual factor loadings against scrambled loadings (rows of $\Lambda$ randomly permuted). Scrambling destroys the question-to-factor mapping while preserving marginal distributions. Across 100 random permutations, the gradient collapsed to near-zero (@fig-scrambled). This confirms that the Bayesian model captures real structure in belief covariance---the specific question-to-factor mapping is meaningful, not arbitrary.

```{python}
#| label: fig-scrambled
#| fig-cap: "Scrambled loadings control. Distribution of gradients across 100 random permutations of the loading matrix (gray histogram) compared to the actual loadings (green line) and human gradient (black dashed line). Scrambling eliminates structured transfer."
#| fig-width: 6
#| fig-height: 4

# Compute actual gradient
model_actual = CommonalityModel(k=5, **UNIFIED_PARAMS)
preds_actual = fast_evaluate(model_actual, nochat_eval)
actual_gradient = (
    (preds_actual[(preds_actual['question_type'] == 'same_domain') & (preds_actual['match_type'] == 'high')]['pred_prob'].mean() -
     preds_actual[(preds_actual['question_type'] == 'same_domain') & (preds_actual['match_type'] == 'low')]['pred_prob'].mean()) -
    (preds_actual[(preds_actual['question_type'] == 'different_domain') & (preds_actual['match_type'] == 'high')]['pred_prob'].mean() -
     preds_actual[(preds_actual['question_type'] == 'different_domain') & (preds_actual['match_type'] == 'low')]['pred_prob'].mean())
)

# Scrambled loadings control
real_loadings = load_factor_loadings(k=5)
np.random.seed(42)
shuffled_grads = []
for seed in range(100):
    np.random.seed(seed)
    perm = np.random.permutation(35)
    params = {k: v for k, v in UNIFIED_PARAMS.items() if k != 'lambda_mix'}
    m = CommonalityModel(k=5, lambda_mix=0.0, loadings=real_loadings[perm, :], **params)
    preds = fast_evaluate(m, nochat_eval)

    same_high = preds[(preds['question_type'] == 'same_domain') & (preds['match_type'] == 'high')]['pred_prob'].mean()
    same_low = preds[(preds['question_type'] == 'same_domain') & (preds['match_type'] == 'low')]['pred_prob'].mean()
    diff_high = preds[(preds['question_type'] == 'different_domain') & (preds['match_type'] == 'high')]['pred_prob'].mean()
    diff_low = preds[(preds['question_type'] == 'different_domain') & (preds['match_type'] == 'low')]['pred_prob'].mean()

    shuffled_grads.append((same_high - same_low) - (diff_high - diff_low))

scrambled_gradient = np.mean(shuffled_grads)
scrambled_std = np.std(shuffled_grads)

# Plot
fig, ax = plt.subplots(figsize=(6, 4))
ax.hist(shuffled_grads, bins=20, color='gray', alpha=0.7, edgecolor='black', label='Scrambled')
ax.axvline(actual_gradient, color='#4d9221', linewidth=2.5, label=f'Actual (k=5)')
ax.axvline(HUMAN_GRADIENT_NC, color='black', linestyle='--', linewidth=2, label='Human')
ax.set_xlabel('Gradient (same - diff)', fontsize=12)
ax.set_ylabel('Count', fontsize=12)
ax.legend(frameon=False)
plt.tight_layout()
plt.savefig(figures_dir / 'scrambled_control.pdf', dpi=300, bbox_inches='tight')
plt.show()

# Statistics
z_score = (actual_gradient - scrambled_gradient) / scrambled_std
print(f'Actual loadings: Gradient = +{actual_gradient:.3f} ({actual_gradient/HUMAN_GRADIENT_NC*100:.0f}% of human)')
print(f'Scrambled loadings: Gradient = +{scrambled_gradient:.3f} +/- {scrambled_std:.3f} ({scrambled_gradient/HUMAN_GRADIENT_NC*100:.0f}% of human)')
print(f'z = {z_score:.2f}, p < .001')
```

## Question Property Norms {#sec-norms}

To characterize the stimulus set, an independent sample (N = 15–27 per dimension) rated each question on several dimensions: how personally revealing, how self-relevant, how divisive or controversial, how deep, how much one would learn from the answer, how interesting, how likely one would discuss the topic, and expected population agreement. Ratings were made on 0–100 scales.

```{python}
#| label: fig-norms
#| fig-cap: "Normative ratings of question properties by domain. Cells show mean ratings (0–100 scale) across questions within each domain. Domains are sorted by mean personal informativeness rating."
#| fig-width: 10
#| fig-height: 5

# Re-import after R chunks (reticulate loses Python state)
import pandas as pd
import numpy as np
import matplotlib.pyplot as plt
from pathlib import Path

# Re-establish paths
base_dir = Path.cwd().resolve()
while base_dir.name and not (base_dir / 'data').exists():
    base_dir = base_dir.parent
data_dir = base_dir / 'data'

# Load questions with norms
questions = pd.read_csv(data_dir / 'questions.csv')

# Compute domain-level means
norm_cols = ['norm_personal', 'norm_relevant', 'norm_divisive', 'norm_deep', 'norm_learn', 'norm_interest', 'norm_likely', 'norm_same']
domain_norms = questions.groupby('domain')[norm_cols].mean()

# Rename for display
domain_norms.index = domain_norms.index.str.replace('arbitrary', 'Lifestyle').str.capitalize()
domain_norms.columns = ['Personal', 'Relevant', 'Divisive', 'Deep', 'Learn', 'Interest', 'Likely', 'Agreement']

# Sort by Personal (most theoretically relevant)
domain_norms = domain_norms.sort_values('Personal', ascending=False)

# Create heatmap
fig, ax = plt.subplots(figsize=(7, 5))
plt.rcParams.update({'font.size': 11, 'font.family': 'Helvetica Neue'})

im = ax.imshow(domain_norms.values, cmap='YlOrRd', aspect='auto', vmin=15, vmax=75)

# Labels
ax.set_xticks(range(len(domain_norms.columns)))
ax.set_xticklabels(domain_norms.columns, fontsize=11)
ax.set_yticks(range(len(domain_norms.index)))
ax.set_yticklabels(domain_norms.index, fontsize=11)

# Add values to cells
for i in range(len(domain_norms.index)):
    for j in range(len(domain_norms.columns)):
        val = domain_norms.iloc[i, j]
        color = 'white' if val > 50 else 'black'
        ax.text(j, i, f'{val:.0f}', ha='center', va='center', fontsize=10, color=color)

# Colorbar
cbar = plt.colorbar(im, ax=ax, shrink=0.8, pad=0.02)
cbar.set_label('Mean Rating', fontsize=11)

ax.set_xlabel('Question Property', fontsize=12)
ax.set_ylabel('Domain', fontsize=12)

plt.tight_layout()
plt.show()
```

@fig-norms shows domain-level means for key question properties. Religion and identity questions were rated as most personally revealing, while politics and religion were rated as most divisive. Lifestyle and preference questions were rated as least deep and least controversial. This variation in perceived question properties aligns with the differential transfer effects observed across domains and validates that experimenter-assigned domain labels capture meaningful psychological distinctions in how people perceive different types of beliefs.

## Domain-Specific Regression Results {#sec-domain-models}

@tbl-domain-models presents the full results of the 42 logistic regression models examining the effect of real vs. imagined interaction on commonality expectations, broken down by focal domain, generalization type, and stance condition. Positive coefficients indicate that conversation increased commonality expectations relative to observation-only.

```{python}
#| output: false
# Reload processed data (saved in load-data chunk)
import pandas as pd
df_pd = pd.read_csv('/tmp/df_pd_processed.csv', low_memory=False)
df_pd.to_csv('/tmp/main_df.csv', index=False)
```

```{r}
#| label: tbl-domain-models
#| tbl-cap: "Domain-specific regression coefficients for effect of conversation (real vs. imagined interaction) on commonality expectations. Asterisks indicate significance after FDR correction (* p < .05, ** p < .01, *** p < .001)."

domain_df <- read.csv('/tmp/main_df.csv')
domain_df$experiment <- factor(domain_df$experiment, levels = c('no-chat', 'chat'))
contrasts(domain_df$experiment) <- cbind(c(-0.5, 0.5))

fit_model <- function(data, cat) {
    tryCatch({
        if (cat == 'sameQ') {
            m <- glm(predictShared ~ experiment, data = data, family = binomial())
        } else {
            m <- glmer(predictShared ~ experiment + (1|pid), data = data, family = binomial(),
                       control = glmerControl(optimizer = 'bobyqa', optCtrl = list(maxfun = 100000)))
        }
        tibble(beta = coef(summary(m))[2, 1], pVal = coef(summary(m))[2, 4])
    }, error = function(e) tibble(beta = NA_real_, pVal = NA_real_))
}

results <- domain_df %>%
    mutate(matchedDomain = gsub('arbitrary', 'Lifestyle', matchedDomain)) %>%
    mutate(matchedDomain = tools::toTitleCase(matchedDomain)) %>%
    group_by(matchedDomain, category, stance) %>%
    nest() %>%
    mutate(res = map2(data, category, fit_model)) %>%
    select(-data) %>%
    unnest(res) %>%
    ungroup() %>%
    mutate(pVal_fdr = p.adjust(pVal, method = 'fdr'))

# Add significance stars
results <- results %>%
    mutate(sig = case_when(
        pVal_fdr < 0.001 ~ '***',
        pVal_fdr < 0.01 ~ '**',
        pVal_fdr < 0.05 ~ '*',
        TRUE ~ ''
    ))

# Format for display - pivot and reorder columns correctly
results_wide <- results %>%
    mutate(beta_sig = sprintf('%.2f%s', beta, sig)) %>%
    select(matchedDomain, category, stance, beta_sig) %>%
    pivot_wider(names_from = c(stance, category), values_from = beta_sig) %>%
    select(matchedDomain,
           opposing_sameQ, opposing_sameD, opposing_diffD,
           shared_sameQ, shared_sameD, shared_diffD)

knitr::kable(results_wide, col.names = c('Domain', 'Opp-Focal', 'Opp-Same', 'Opp-Diff', 'Shared-Focal', 'Shared-Same', 'Shared-Diff'))

# Summary stats
n_pos <- sum(results$beta > 0, na.rm = TRUE)
n_sig_pos <- sum(results$pVal_fdr < 0.05 & results$beta > 0, na.rm = TRUE)
n_sig_neg <- sum(results$pVal_fdr < 0.05 & results$beta < 0, na.rm = TRUE)

cat(sprintf('\nTotal models: %d\n', nrow(results)))
cat(sprintf('Positive coefficients: %d (%.1f%%)\n', n_pos, 100 * n_pos / nrow(results)))
cat(sprintf('Significant after FDR: %d positive, %d negative\n', n_sig_pos, n_sig_neg))
```

Over 95% of coefficients were positive, and every significant effect after FDR correction was in the positive direction (conversation increased commonality inference).

## LLM Prompt Template {#sec-llm-prompt}

The following prompt was used to elicit predictions from Gemini 2.5 Pro:

```
Predict how two people (Cat and Dog) would respond to survey questions
based on their conversation.

Likert scale:
1 = Definitely Not / Strongly Disagree
2 = Probably Not / Disagree
3 = Unsure / Neutral
4 = Probably Yes / Agree
5 = Definitely Yes / Strongly Agree

One question is marked [DISCUSSED] - they talked about this topic.
For other questions, infer from the conversation.

Base your predictions on:
- General population tendencies (as a starting point)
- Evidence from the conversation (update based on what they discuss)
- When conversation evidence is weak or absent, rely more on population priors

=== CONVERSATION ===
{conversation transcript}

=== QUESTIONS ===
{questions list with [DISCUSSED] marker}

=== TASK ===
Predict probability distributions for BOTH participants' responses
to ALL 35 questions.

Return JSON with this structure:
{
  "cat_predictions": {"0": {"1": p1, ..., "5": p5}, ...},
  "dog_predictions": {"0": {"1": p1, ..., "5": p5}, ...}
}
```

For the prior baseline (no conversation), we used a simplified prompt asking the model to predict responses for a randomly selected person based only on "general population tendencies" and "the nature of the question," with no conversation content.

## LLM Timecourse Results {#sec-llm-pshared}

@tbl-llm-pshared presents LLM-predicted P(shared) at conversation end compared to human judgments.

```{python}
#| label: tbl-llm-pshared
#| tbl-cap: "LLM-predicted P(shared) at conversation end (180s) compared to human judgments, by stance condition and question type."

# Re-import after R chunk (knitr breaks Python session)
import pandas as pd
from pathlib import Path
base_dir = Path.cwd().resolve()
while base_dir.name and not (base_dir / 'data').exists():
    base_dir = base_dir.parent

llm_dir = base_dir / 'data' / 'llm_results'
data_dir = base_dir / 'data'

# Load P(shared) timecourse - use final time bin
pshared_df = pd.read_csv(llm_dir / 'pshared_timecourse.csv')
pshared_hl = pshared_df[pshared_df['match_type'].isin(['high', 'low'])]
final_bin = pshared_hl['time_bin'].max()
pshared_final = pshared_hl[pshared_hl['time_bin'] == final_bin]

# Compute LLM P(shared) by stance and question category
llm_pshared = {}
for mt in ['high', 'low']:
    for cat in ['matched', 'same_domain', 'different_domain']:
        subset = pshared_final[(pshared_final['match_type'] == mt) &
                               (pshared_final['question_category'] == cat)]
        if len(subset) > 0:
            llm_pshared[(mt, cat)] = subset['predicted_agreement'].mean()

# Load human behavioral data for comparison
responses_df = pd.read_csv(data_dir / 'responses.csv', low_memory=False)
chat_data = responses_df[responses_df['experiment'] == 'chat']

# Compute human P(shared) by stance and question type
human_pshared = {}
cat_to_qt = {'matched': 'observed', 'same_domain': 'same_domain', 'different_domain': 'different_domain'}
for mt in ['high', 'low']:
    for cat, qt in cat_to_qt.items():
        subset = chat_data[(chat_data['match_type'] == mt) & (chat_data['question_type'] == qt)]
        if len(subset) > 0:
            human_pshared[(mt, cat)] = subset['participant_binary_prediction'].mean()

# Create table matching LaTeX format
rows = []
for mt, stance_label in [('high', 'Shared'), ('low', 'Opposing')]:
    for cat, cat_label in [('matched', 'Focal (discussed)'), ('same_domain', 'Same domain'), ('different_domain', 'Different domain')]:
        llm_val = llm_pshared.get((mt, cat), float('nan'))
        human_val = human_pshared.get((mt, cat), float('nan'))
        rows.append({
            'Stance': stance_label,
            'Question Type': cat_label,
            'LLM P(shared)': f'{llm_val:.2f}',
            'Human P(shared)': f'{human_val:.2f}'
        })

table_df = pd.DataFrame(rows)
print(table_df.to_markdown(index=False))
```

For shared stance pairs, LLM predictions closely matched human judgments across all question types, with the generalization gradient clearly visible (focal > same domain > different domain). For opposing stance pairs, the LLM correctly predicted low agreement on the focal question but was more extreme than human judgments; same-domain and different-domain predictions were well-calibrated.

## Survey Questions by Domain {#sec-stimuli}

@tbl-stimuli presents the complete set of 35 survey questions organized by domain.

```{python}
#| label: tbl-stimuli
#| tbl-cap: "Survey questions by domain."

# Uses base_dir from previous chunk
data_dir = base_dir / 'data'
questions_df = pd.read_csv(data_dir / 'questions.csv')
questions_df['domain'] = questions_df['domain'].replace('arbitrary', 'Lifestyle').str.capitalize()

# Group by domain
stimuli_by_domain = questions_df.groupby('domain')['text'].apply(lambda x: ' / '.join(x)).reset_index()
stimuli_by_domain.columns = ['Domain', 'Questions']

print(stimuli_by_domain.to_markdown(index=False))
```
