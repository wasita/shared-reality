---
title: "Supplementary Information"
author: "Reproducibility Code"
format:
  html:
    code-fold: true
    toc: true
    toc-depth: 3
  pdf:
    documentclass: article
    number-sections: false
engine: knitr
execute:
  warning: false
  message: false
  cache: true
knitr:
  opts_chunk:
    message: false
    warning: false
    cache: true
---

```{r}
#| label: setup
#| include: false
Sys.setenv(RETICULATE_PYTHON = "/Users/rxdh/miniconda3/bin/python3")
library(reticulate)
use_python("/Users/rxdh/miniconda3/bin/python3", required = TRUE)
library(lme4)
library(lmerTest)
library(dplyr)
library(tidyr)
library(purrr)
```

```{python}
#| label: python-setup
#| include: false
import sys
from pathlib import Path
import pandas as pd
import numpy as np
import seaborn as sns
import matplotlib.pyplot as plt
from scipy.stats import pearsonr, linregress
from scipy import stats
from statsmodels.stats.multitest import multipletests
import polars as pl
from polars import col, lit

plt.rcParams['font.family'] = 'Helvetica Neue'
sns.set_style('whitegrid')

# Find project root
base_dir = Path.cwd().resolve()
while base_dir.name and not (base_dir / 'data').exists():
    base_dir = base_dir.parent
if not (base_dir / 'data').exists():
    raise ValueError('Could not find project root with data directory')

data_dir = base_dir / 'data'
figures_dir = base_dir / 'outputs' / 'figures'
figures_dir.mkdir(parents=True, exist_ok=True)

sys.path.insert(0, str(base_dir))

lowhigh_palette = ['#648FFF', '#DC267F']  # low (blue), high (pink)
```

```{python}
#| label: load-data
#| include: false

# Load pre-merged behavioral data
df = pl.read_csv(data_dir / 'responses.csv')
df = df.filter(~col('matchedTolerance').is_null())

# Derived variables
df = df.with_columns([
    pl.when(col('matchedTolerance') <= 1).then(lit('shared')).otherwise(lit('opposing')).alias('stance'),
    pl.when(col('matchedQuestion') == col('preChatQuestion')).then(lit('sameQ'))
     .when(col('matchedDomain') == col('preChatDomain')).then(lit('sameD'))
     .otherwise(lit('diffD')).alias('category'),
    (col('postChatResponse') - col('preChatResponse')).abs().alias('distance')
]).rename({'groupId': 'dyadId'})

# Convert to pandas with numeric contrasts
df_pd = df.to_pandas()
df_pd['stance_num'] = df_pd['stance'].map({'opposing': -1, 'shared': 1})
df_pd['category_num'] = df_pd['category'].map({'sameQ': 1, 'sameD': 0, 'diffD': -1}).astype(float)
df_pd['experiment_num'] = df_pd['experiment'].map({'no-chat': -1, 'chat': 1})
df_pd['distance_num'] = df_pd['distance'].map({0: 2, 1: 1, 2: 0, 3: -1, 4: -2})

# Save processed data for later chunks (Python state can be lost)
df_pd.to_csv('/tmp/df_pd_processed.csv', index=False)

# Inference subset (excludes observed question)
inf_df = df_pd[df_pd['question'] != df_pd['matchedIdx']]
```

## Internal Consistency of Commonality Judgments {#sec-distance}

To verify that participants' binary commonality judgments aligned with their continuous predictions about their partner's beliefs, we calculated the distance between each participant's own (pre-interaction) response and their (post-interaction) prediction of their partner's response for every survey question. We reasoned that participants should expect commonality when they predicted their partner held similar views to their own, and expect less commonality when they predicted divergent views.

A logistic mixed-effects model predicted item-level commonality expectations from this predicted distance, interaction type, stance condition, and all interactions, with random intercepts for participants. As expected, greater distance between one's own response and predicted partner response significantly reduced the likelihood of expecting commonality ($\beta = -1.32$, 95% CI $[-1.36, -1.29]$, $p < 0.001$; @fig-distance-commonality). Participants were most likely to expect commonality when they predicted their partner held identical views (distance $= 0$), with expectations declining monotonically as predicted distance increased.

Interaction type moderated this relationship ($\beta = -0.32$, 95% CI $[-0.36, -0.29]$, $p < 0.001$): participants in real conversations maintained higher commonality expectations even when predicting substantial disagreement, whereas those in imagined interactions required closer predicted alignment before expecting shared views. This pattern held across all seven topic domains, suggesting that conversation relaxed thresholds for inferring commonality.

```{python}
#| label: fig-distance-commonality
#| fig-cap: "Commonality expectations as a function of predicted distance between partner and self responses. Participants expected less commonality as the predicted distance increased. Chat participants (right) maintained higher commonality expectations than observation-only participants (left) across all distance levels. Fill color indicates stance condition (pink = shared, blue = opposing). Error bars are 95% bootstrapped confidence intervals."
#| fig-width: 10
#| fig-height: 5

# Aggregate by participant first
inf_grouped = inf_df.groupby(['experiment', 'stance', 'pid', 'distance'])['predictShared'].mean().reset_index()

# Bootstrap CI function
def bootstrap_ci(data, n_boot=1000, ci=95):
    boot_means = [np.random.choice(data, size=len(data), replace=True).mean() for _ in range(n_boot)]
    lower = np.percentile(boot_means, (100 - ci) / 2)
    upper = np.percentile(boot_means, 100 - (100 - ci) / 2)
    return lower, upper

# Compute means and CIs
summary_data = []
for exp in ['no-chat', 'chat']:
    for stance in ['opposing', 'shared']:
        for dist in range(5):
            subset = inf_grouped[(inf_grouped['experiment'] == exp) &
                                  (inf_grouped['stance'] == stance) &
                                  (inf_grouped['distance'] == dist)]['predictShared']
            if len(subset) > 0:
                mean_val = subset.mean()
                ci_low, ci_high = bootstrap_ci(subset.values)
                summary_data.append({
                    'experiment': exp, 'stance': stance, 'distance': dist,
                    'mean': mean_val, 'ci_low': ci_low, 'ci_high': ci_high
                })

summary_df = pd.DataFrame(summary_data)

# Create figure
fig, axes = plt.subplots(1, 2, figsize=(10, 5), sharey=True)
plt.rcParams.update({'font.size': 12})

exp_labels = {'no-chat': 'Imagined', 'chat': 'Real'}
stance_colors = {'opposing': '#648FFF', 'shared': '#DC267F'}  # blue, pink

for i, exp in enumerate(['no-chat', 'chat']):
    ax = axes[i]
    exp_data = summary_df[summary_df['experiment'] == exp]

    bar_width = 0.35
    x = np.arange(5)

    for j, stance in enumerate(['opposing', 'shared']):
        stance_data = exp_data[exp_data['stance'] == stance].sort_values('distance')
        offset = -bar_width/2 if stance == 'opposing' else bar_width/2

        bars = ax.bar(x + offset, stance_data['mean'], bar_width,
                      color=stance_colors[stance], alpha=0.8,
                      label=stance.capitalize() if i == 1 else '')

        # Error bars - no caps
        ax.errorbar(x + offset, stance_data['mean'],
                    yerr=[stance_data['mean'] - stance_data['ci_low'],
                          stance_data['ci_high'] - stance_data['mean']],
                    fmt='none', color='black', capsize=0, linewidth=1.5)

    ax.set_xlabel('Predicted Distance\n(|Prediction for Partner − Own|)', fontsize=14)
    ax.set_xticks(x)
    ax.set_xticklabels(['0', '1', '2', '3', '4'], fontsize=12)
    ax.set_ylim([0, 1])
    ax.set_yticks([0, 0.2, 0.4, 0.6, 0.8, 1.0])
    ax.text(-0.1, 1.05, chr(65 + i), transform=ax.transAxes, fontsize=16, fontweight='bold', va='bottom')
    ax.set_title(exp_labels[exp], fontsize=14, fontweight='bold')
    ax.tick_params(labelsize=11)
    ax.grid(False)
    ax.spines['top'].set_visible(False)
    ax.spines['right'].set_visible(False)

    if i == 0:
        ax.set_ylabel('Commonality Expectation', fontsize=14)
    if i == 1:
        ax.legend(title='Stance', frameon=False, loc='upper right', fontsize=11)

plt.tight_layout()
plt.savefig(figures_dir / 'figure_s1.pdf', dpi=300, bbox_inches='tight')
plt.show()
```

```{python}
#| output: false
inf_df.to_csv('/tmp/inf_df.csv', index=False)
```

```{r}
#| label: distance-model
inf_df <- read.csv('/tmp/inf_df.csv')
model <- glmer(predictShared ~ distance_num * experiment_num * stance_num + (1|pid),
               data = inf_df, family = binomial)
summary(model)
confint(model, method = "Wald")
```

## Factor Structure of Belief Space {#sec-factor}

We computed factor loadings via eigendecomposition of the 35-question belief correlation matrix. @fig-factor-structure panel A shows the scree plot comparing observed eigenvalues against the 95th percentile from parallel analysis (eigenvalues computed from random data with the same dimensions). The first five components exceeded the parallel analysis threshold, suggesting $k = 5$ factors. These five factors explained 36% of total variance.

@fig-factor-structure panel B displays the absolute loadings for the first five factors, organized by experimenter-assigned domain. Religion and politics questions loaded strongly on single factors (PC1 and PC3, respectively), indicating tight clustering in belief space. Lifestyle and background questions showed more diffuse loadings across multiple factors. This variation in factor structure predicts domain-specific generalization patterns: domains with coherent loadings should support stronger within-domain transfer.

```{python}
#| label: fig-factor-structure
#| fig-cap: "Factor structure of belief space. (A) Scree plot comparing observed eigenvalues (solid) against the 95th percentile from parallel analysis (dashed). The first five components exceed the threshold. (B) Absolute factor loadings for the first five principal components, grouped by domain. Religion and politics questions load strongly on single factors, while lifestyle and background questions show more diffuse loadings across factors."
#| fig-width: 12
#| fig-height: 5

# Re-import after R chunks
import sys
import pandas as pd
import numpy as np
import matplotlib.pyplot as plt
import seaborn as sns
from pathlib import Path

base_dir = Path('/Users/rxdh/Repos/shared-reality')
data_dir = base_dir / 'data'
figures_dir = base_dir / 'outputs' / 'figures'
sys.path.insert(0, str(base_dir))

from models.model import load_factor_loadings

# Load full loadings to compute eigenvalues for scree plot
loadings_full = load_factor_loadings(k=35)
eigenvalues = np.sum(loadings_full ** 2, axis=0)  # Variance per factor

# Load k=5 for heatmap
loadings_k5 = load_factor_loadings(k=5)
loadings_k5[:, 2] *= -1  # Flip factor 3 sign for interpretability
var_explained_k5 = np.sum(eigenvalues[:5]) / np.sum(eigenvalues) * 100

# Reorder domains for interpretability
domain_order = ['Religion', 'Politics', 'Background', 'Morality', 'Lifestyle', 'Identity', 'Preferences']
original_ranges = {
    'Lifestyle': (0, 5), 'Background': (5, 10), 'Identity': (10, 15),
    'Morality': (15, 20), 'Politics': (20, 25), 'Preferences': (25, 30), 'Religion': (30, 35)
}

# Build reordering index
reorder_idx = []
for domain in domain_order:
    start, end = original_ranges[domain]
    domain_qs = list(range(start, end))
    if domain == 'Preferences':
        domain_qs.sort(key=lambda q: -loadings_k5[q, 4])
    else:
        domain_qs.sort(key=lambda q: (np.argmax(np.abs(loadings_k5[q])), -np.max(np.abs(loadings_k5[q]))))
    reorder_idx.extend(domain_qs)

loadings_reord = loadings_k5[reorder_idx, :]

# Load questions
questions_df = pd.read_csv(data_dir / 'questions.csv')
questions_df['domain'] = questions_df['domain'].replace('arbitrary', 'Lifestyle').str.capitalize()
reord_labels = [questions_df.iloc[i]['text'][:40] + '...' if len(questions_df.iloc[i]['text']) > 40
                else questions_df.iloc[i]['text'] for i in reorder_idx]

# Parallel analysis
n_parallel = 1000
responses_df = pd.read_csv(data_dir / 'responses.csv', low_memory=False)
n_obs = responses_df['pid'].nunique()
parallel_eigs = np.zeros((n_parallel, 35))
for i in range(n_parallel):
    random_data = np.random.randn(n_obs, 35)
    cov = np.corrcoef(random_data.T)
    parallel_eigs[i] = np.linalg.eigvalsh(cov)[::-1]
parallel_95 = np.percentile(parallel_eigs, 95, axis=0)

# Create figure
fig, (ax1, ax2) = plt.subplots(1, 2, figsize=(12, 5), gridspec_kw={'width_ratios': [1, 1.5]})

# Panel A: Scree plot
ax1.plot(range(1, 36), eigenvalues, 'o-', color='#2166ac', linewidth=2, markersize=6, label='Observed')
ax1.plot(range(1, 36), parallel_95, 's--', color='#b2182b', linewidth=1.5, markersize=4, label='95th percentile\n(parallel analysis)')
ax1.axvline(5.5, color='gray', linestyle=':', alpha=0.6)
ax1.set_xlabel('Component', fontsize=12)
ax1.set_ylabel('Eigenvalue', fontsize=12)
ax1.set_xlim(0, 20)
ax1.legend(frameon=False, fontsize=10)
ax1.text(-0.15, 1.05, 'A', transform=ax1.transAxes, fontsize=16, fontweight='bold')
ax1.text(6, eigenvalues[4] + 0.3, 'k=5', fontsize=11, color='gray')

# Panel B: Heatmap
im = ax2.imshow(np.abs(loadings_reord), cmap='Reds', aspect='auto', vmin=0, vmax=0.8)

# Domain boundaries
domain_boundaries = []
cumsum = 0
for domain in domain_order:
    start, end = original_ranges[domain]
    domain_boundaries.append(cumsum + (end - start) / 2 - 0.5)
    cumsum += end - start

for i, (domain, ypos) in enumerate(zip(domain_order, domain_boundaries)):
    ax2.text(-0.5, ypos, domain, ha='right', va='center', fontsize=10, fontweight='bold')

# Add horizontal lines between domains
cumsum = 0
for domain in domain_order[:-1]:
    start, end = original_ranges[domain]
    cumsum += end - start
    ax2.axhline(cumsum - 0.5, color='white', linewidth=2)

ax2.set_xticks(range(5))
ax2.set_xticklabels([f'F{i+1}' for i in range(5)], fontsize=11)
ax2.set_yticks([])
ax2.set_xlabel('Factor', fontsize=12)
ax2.text(-0.05, 1.05, 'B', transform=ax2.transAxes, fontsize=16, fontweight='bold')

cbar = plt.colorbar(im, ax=ax2, shrink=0.6, label='|Loading|')

plt.tight_layout()
plt.savefig(figures_dir / 'figure_s2.pdf', dpi=300, bbox_inches='tight')
plt.show()

print(f'Variance explained by k=5 factors: {var_explained_k5:.1f}%')
```

## Parameter Sensitivity Analyses {#sec-sensitivity}

### Factor dimensionality ($k$)

We evaluated model performance across factor dimensionalities from $k = 1$ (first principal component) to $k = 35$ (full covariance matrix). @fig-k-sweep shows that performance peaked at low dimensionality and degraded at high $k$. This pattern confirms that structured compression (low $k$) captures transferable commonalities better than full covariance. High-$k$ models overfit to noise and idiosyncratic correlations, diluting the domain-specific transfer signal. The optimal $k$ from behavioral fit closely matches parallel analysis, providing independent validation of the factor structure.

```{python}
#| label: fig-k-sweep
#| fig-cap: "Model performance varies with factor dimensionality. Transfer effects as a function of k: same-domain (blue), different-domain (red), and gradient (green). Dashed lines show human targets."
#| fig-width: 7
#| fig-height: 5

from models.model import CommonalityModel, load_evaluation_data

# Load evaluation data
data = load_evaluation_data()
nochat_data = data[data['experiment'] == 'no-chat']

# Compute human rates for no-chat condition
NOCHAT_HUMAN = {}
for qt in ['same_domain', 'different_domain']:
    for mt in ['high', 'low']:
        cell = nochat_data[(nochat_data['question_type'] == qt) & (nochat_data['match_type'] == mt)]
        NOCHAT_HUMAN[(qt, mt)] = cell['participant_binary_prediction'].mean()

# Load fitted parameters (same as model_analyses.qmd)
import json
params_path = base_dir / 'models' / 'fitted_params.json'
with open(params_path) as f:
    fitted_cache = json.load(f)
UNIFIED_PARAMS = {**fitted_cache['unified_params'], 'lambda_mix': 0.0}

# Prepare evaluation data
nochat_eval = nochat_data[['pid', 'question', 'question_type', 'match_type',
                            'preChatResponse', 'partner_response', 'matchedIdx',
                            'participant_binary_prediction', 'is_matched', 'experiment']].copy()

def fast_evaluate(model, eval_data):
    """Fast evaluation on prepared data."""
    results = []
    for pid in eval_data['pid'].unique():
        subj = eval_data[eval_data['pid'] == pid]
        matched = subj[subj['is_matched'] == True]
        if len(matched) == 0:
            continue
        obs_q = int(matched['matchedIdx'].iloc[0]) - 1
        r_partner = matched['partner_response'].iloc[0]
        if pd.isna(r_partner):
            continue
        r_self = np.zeros(35)
        for _, row in subj.iterrows():
            r_self[int(row['question']) - 1] = row['preChatResponse']
        preds = model.predict(obs_q, float(r_partner), r_self)
        for _, row in subj.iterrows():
            results.append({
                'pid': pid,
                'question_type': row['question_type'],
                'match_type': row['match_type'],
                'pred_prob': preds[int(row['question']) - 1],
                'actual': row['participant_binary_prediction'],
            })
    return pd.DataFrame(results)

# K values to sweep
k_values = [1, 2, 3, 4, 5, 6, 7, 8, 10, 15, 20, 25, 30, 35]

# Compute metrics for each k
k_sweep_results = []
for k in k_values:
    model = CommonalityModel(k=k, **UNIFIED_PARAMS)
    preds = fast_evaluate(model, nochat_eval)

    # Effects by question type and match type
    same_high = preds[(preds['question_type'] == 'same_domain') & (preds['match_type'] == 'high')]['pred_prob'].mean()
    same_low = preds[(preds['question_type'] == 'same_domain') & (preds['match_type'] == 'low')]['pred_prob'].mean()
    diff_high = preds[(preds['question_type'] == 'different_domain') & (preds['match_type'] == 'high')]['pred_prob'].mean()
    diff_low = preds[(preds['question_type'] == 'different_domain') & (preds['match_type'] == 'low')]['pred_prob'].mean()

    same_effect = same_high - same_low
    diff_effect = diff_high - diff_low
    gradient = same_effect - diff_effect

    k_sweep_results.append({
        'k': k,
        'same_effect': same_effect,
        'diff_effect': diff_effect,
        'gradient': gradient,
    })

k_sweep = pd.DataFrame(k_sweep_results)

# Compute human targets
HUMAN_SAME_NC = NOCHAT_HUMAN[('same_domain', 'high')] - NOCHAT_HUMAN[('same_domain', 'low')]
HUMAN_DIFF_NC = NOCHAT_HUMAN[('different_domain', 'high')] - NOCHAT_HUMAN[('different_domain', 'low')]
HUMAN_GRADIENT_NC = HUMAN_SAME_NC - HUMAN_DIFF_NC

k_sweep['gradient_error'] = np.abs(k_sweep['gradient'] - HUMAN_GRADIENT_NC)

# Plot
plt.rcParams.update({
    'font.size': 14,
    'axes.labelsize': 16,
    'xtick.labelsize': 13,
    'ytick.labelsize': 13,
    'legend.fontsize': 12,
})

fig, ax = plt.subplots(figsize=(7, 5))
k = k_sweep['k'].values

ax.plot(k, k_sweep['same_effect'], 'o-', color='#2166ac', linewidth=2.5,
        markersize=8, label='Same-domain effect')
ax.plot(k, k_sweep['diff_effect'], 's-', color='#b2182b', linewidth=2.5,
        markersize=8, label='Different-domain effect')
ax.plot(k, k_sweep['gradient'], '^-', color='#4d9221', linewidth=2.5,
        markersize=8, label='Gradient (same - diff)')

# Human targets as dashed lines
ax.hlines(HUMAN_SAME_NC, 0, max(k), color='#2166ac', linestyle='--', alpha=0.6, linewidth=1.5)
ax.hlines(HUMAN_DIFF_NC, 0, max(k), color='#b2182b', linestyle='--', alpha=0.6, linewidth=1.5)
ax.hlines(HUMAN_GRADIENT_NC, 0, max(k), color='#4d9221', linestyle='--', alpha=0.6, linewidth=1.5)

ax.set_xlabel('Number of factors (k)')
ax.set_ylabel('Effect magnitude')
ax.legend(loc='upper right', frameon=False)
ax.set_xlim(0, max(k) + 1)
ax.axhline(0, color='gray', linewidth=0.5, alpha=0.5)

# Highlight optimal k
best_k = k_sweep.loc[k_sweep['gradient_error'].idxmin(), 'k']
ax.axvline(best_k, color='gray', linestyle=':', alpha=0.6, linewidth=1.5)
ax.text(best_k + 0.5, 0.01, f'k={int(best_k)}', fontsize=12, color='gray')

plt.tight_layout()
plt.savefig(figures_dir / 'figure_s3.pdf', dpi=300, bbox_inches='tight')
plt.show()

# Summary statistics
print(f'Human gradient: {HUMAN_GRADIENT_NC:.4f}')
print(f'Optimal k: {int(best_k)}')
for k_val in [1, 5, 35]:
    if k_val in k_sweep['k'].values:
        row = k_sweep[k_sweep['k'] == k_val].iloc[0]
        pct = row['gradient'] / HUMAN_GRADIENT_NC * 100
        print(f"  k={k_val}: Gradient = +{row['gradient']:.3f} ({pct:.0f}% of human)")
```

### Bayesian vs. egocentric mixture weight ($\lambda$)

To assess whether people blend Bayesian and egocentric strategies, we fit a mixture weight $\lambda \in [0, 1]$ that interpolates between the pure Bayesian model ($\lambda = 0$) and pure egocentric model ($\lambda = 1$):

$$P(\text{match}) = (1 - \lambda) \cdot P_{\text{Bayesian}}(\text{match}) + \lambda \cdot P_{\text{egocentric}}(\text{match})$$

Performance degraded monotonically as $\lambda$ increased. The pure Bayesian model substantially outperformed all alternatives. There was no evidence that adding egocentric projection improved upon pure Bayesian inference.

```{python}
#| label: lambda-sweep

# Lambda sweep
lambda_values = [0.0, 0.25, 0.5, 0.75, 1.0]
lambda_results = []

for lam in lambda_values:
    model = CommonalityModel(k=5, lambda_mix=lam, **{k: v for k, v in UNIFIED_PARAMS.items() if k != 'lambda_mix'})
    preds = fast_evaluate(model, nochat_eval)

    same_high = preds[(preds['question_type'] == 'same_domain') & (preds['match_type'] == 'high')]['pred_prob'].mean()
    same_low = preds[(preds['question_type'] == 'same_domain') & (preds['match_type'] == 'low')]['pred_prob'].mean()
    diff_high = preds[(preds['question_type'] == 'different_domain') & (preds['match_type'] == 'high')]['pred_prob'].mean()
    diff_low = preds[(preds['question_type'] == 'different_domain') & (preds['match_type'] == 'low')]['pred_prob'].mean()

    gradient = (same_high - same_low) - (diff_high - diff_low)
    lambda_results.append({'lambda': lam, 'gradient': gradient})

lambda_df = pd.DataFrame(lambda_results)

print('Lambda sweep results:')
for _, row in lambda_df.iterrows():
    pct = row['gradient'] / HUMAN_GRADIENT_NC * 100
    label = 'pure Bayesian' if row['lambda'] == 0 else ('pure egocentric' if row['lambda'] == 1 else 'mixture')
    print(f"  lambda={row['lambda']:.2f} ({label}): Gradient = +{row['gradient']:.3f} ({pct:.0f}% of human)")
```

## Scrambled Loadings Control {#sec-scrambled}

To verify that model performance depends on veridical belief structure, we compared the actual factor loadings against scrambled loadings (rows of $\Lambda$ randomly permuted). Scrambling destroys the question-to-factor mapping while preserving marginal distributions. Across 100 random permutations, the gradient collapsed to near-zero (@fig-scrambled). This confirms that the Bayesian model captures real structure in belief covariance---the specific question-to-factor mapping is meaningful, not arbitrary.

```{python}
#| label: fig-scrambled
#| fig-cap: "Scrambled loadings control. Distribution of gradients across 100 random permutations of the loading matrix (gray histogram) compared to the actual loadings (green line) and human gradient (black dashed line). Scrambling eliminates structured transfer."
#| fig-width: 6
#| fig-height: 4

# Compute actual gradient
model_actual = CommonalityModel(k=5, **UNIFIED_PARAMS)
preds_actual = fast_evaluate(model_actual, nochat_eval)
actual_gradient = (
    (preds_actual[(preds_actual['question_type'] == 'same_domain') & (preds_actual['match_type'] == 'high')]['pred_prob'].mean() -
     preds_actual[(preds_actual['question_type'] == 'same_domain') & (preds_actual['match_type'] == 'low')]['pred_prob'].mean()) -
    (preds_actual[(preds_actual['question_type'] == 'different_domain') & (preds_actual['match_type'] == 'high')]['pred_prob'].mean() -
     preds_actual[(preds_actual['question_type'] == 'different_domain') & (preds_actual['match_type'] == 'low')]['pred_prob'].mean())
)

# Scrambled loadings control
real_loadings = load_factor_loadings(k=5)
np.random.seed(42)
shuffled_grads = []
for seed in range(100):
    np.random.seed(seed)
    perm = np.random.permutation(35)
    params = {k: v for k, v in UNIFIED_PARAMS.items() if k != 'lambda_mix'}
    m = CommonalityModel(k=5, lambda_mix=0.0, loadings=real_loadings[perm, :], **params)
    preds = fast_evaluate(m, nochat_eval)

    same_high = preds[(preds['question_type'] == 'same_domain') & (preds['match_type'] == 'high')]['pred_prob'].mean()
    same_low = preds[(preds['question_type'] == 'same_domain') & (preds['match_type'] == 'low')]['pred_prob'].mean()
    diff_high = preds[(preds['question_type'] == 'different_domain') & (preds['match_type'] == 'high')]['pred_prob'].mean()
    diff_low = preds[(preds['question_type'] == 'different_domain') & (preds['match_type'] == 'low')]['pred_prob'].mean()

    shuffled_grads.append((same_high - same_low) - (diff_high - diff_low))

scrambled_gradient = np.mean(shuffled_grads)
scrambled_std = np.std(shuffled_grads)

# Plot
fig, ax = plt.subplots(figsize=(6, 4))
ax.hist(shuffled_grads, bins=20, color='gray', alpha=0.7, edgecolor='black', label='Scrambled')
ax.axvline(actual_gradient, color='#4d9221', linewidth=2.5, label=f'Actual (k=5)')
ax.axvline(HUMAN_GRADIENT_NC, color='black', linestyle='--', linewidth=2, label='Human')
ax.set_xlabel('Gradient (same - diff)', fontsize=12)
ax.set_ylabel('Count', fontsize=12)
ax.legend(frameon=False)
plt.tight_layout()
plt.savefig(figures_dir / 'scrambled_control.pdf', dpi=300, bbox_inches='tight')
plt.show()

# Statistics
z_score = (actual_gradient - scrambled_gradient) / scrambled_std
print(f'Actual loadings: Gradient = +{actual_gradient:.3f} ({actual_gradient/HUMAN_GRADIENT_NC*100:.0f}% of human)')
print(f'Scrambled loadings: Gradient = +{scrambled_gradient:.3f} +/- {scrambled_std:.3f} ({scrambled_gradient/HUMAN_GRADIENT_NC*100:.0f}% of human)')
print(f'z = {z_score:.2f}, p < .001')
```

## Question Property Norms {#sec-norms}

To characterize the stimulus set, an independent sample (N = 15–27 per dimension) rated each question on several dimensions: how personally revealing, how self-relevant, how divisive or controversial, how deep, how much one would learn from the answer, how interesting, how likely one would discuss the topic, and expected population agreement. Ratings were made on 0–100 scales.

```{python}
#| label: fig-norms
#| fig-cap: "Normative ratings of question properties by domain. Cells show mean ratings (0–100 scale) across questions within each domain. Domains are sorted by mean personal informativeness rating."
#| fig-width: 10
#| fig-height: 5

# Re-import after R chunks (reticulate loses Python state)
import pandas as pd
import numpy as np
import matplotlib.pyplot as plt
from pathlib import Path

# Re-establish paths
base_dir = Path.cwd().resolve()
while base_dir.name and not (base_dir / 'data').exists():
    base_dir = base_dir.parent
data_dir = base_dir / 'data'

# Load questions with norms
questions = pd.read_csv(data_dir / 'questions.csv')

# Compute domain-level means
norm_cols = ['norm_personal', 'norm_relevant', 'norm_divisive', 'norm_deep', 'norm_learn', 'norm_interest', 'norm_likely', 'norm_same']
domain_norms = questions.groupby('domain')[norm_cols].mean()

# Rename for display
domain_norms.index = domain_norms.index.str.replace('arbitrary', 'Lifestyle').str.capitalize()
domain_norms.columns = ['Personal', 'Relevant', 'Divisive', 'Deep', 'Learn', 'Interest', 'Likely', 'Agreement']

# Sort by Personal (most theoretically relevant)
domain_norms = domain_norms.sort_values('Personal', ascending=False)

# Create heatmap
fig, ax = plt.subplots(figsize=(7, 5))
plt.rcParams.update({'font.size': 11, 'font.family': 'Helvetica Neue'})

im = ax.imshow(domain_norms.values, cmap='YlOrRd', aspect='auto', vmin=15, vmax=75)

# Labels
ax.set_xticks(range(len(domain_norms.columns)))
ax.set_xticklabels(domain_norms.columns, fontsize=11)
ax.set_yticks(range(len(domain_norms.index)))
ax.set_yticklabels(domain_norms.index, fontsize=11)

# Add values to cells
for i in range(len(domain_norms.index)):
    for j in range(len(domain_norms.columns)):
        val = domain_norms.iloc[i, j]
        color = 'white' if val > 50 else 'black'
        ax.text(j, i, f'{val:.0f}', ha='center', va='center', fontsize=10, color=color)

# Colorbar
cbar = plt.colorbar(im, ax=ax, shrink=0.8, pad=0.02)
cbar.set_label('Mean Rating', fontsize=11)

ax.set_xlabel('Question Property', fontsize=12)
ax.set_ylabel('Domain', fontsize=12)

plt.tight_layout()
plt.show()
```

@fig-norms shows domain-level means for key question properties. Religion and identity questions were rated as most personally revealing, while politics and religion were rated as most divisive. Lifestyle and preference questions were rated as least deep and least controversial. This variation in perceived question properties aligns with the differential transfer effects observed across domains and validates that experimenter-assigned domain labels capture meaningful psychological distinctions in how people perceive different types of beliefs.

## Domain-Specific Regression Results {#sec-domain-models}

@tbl-domain-models presents the full results of the 42 logistic regression models examining the effect of real vs. imagined interaction on commonality expectations, broken down by focal domain, generalization type, and stance condition. Positive coefficients indicate that conversation increased commonality expectations relative to observation-only.

```{python}
#| output: false
# Reload processed data (saved in load-data chunk)
import pandas as pd
df_pd = pd.read_csv('/tmp/df_pd_processed.csv', low_memory=False)
df_pd.to_csv('/tmp/main_df.csv', index=False)
```

```{r}
#| label: tbl-domain-models
#| tbl-cap: "Domain-specific regression coefficients for effect of conversation (real vs. imagined interaction) on commonality expectations. Asterisks indicate significance after FDR correction (* p < .05, ** p < .01, *** p < .001)."

domain_df <- read.csv('/tmp/main_df.csv')
domain_df$experiment <- factor(domain_df$experiment, levels = c('no-chat', 'chat'))
contrasts(domain_df$experiment) <- cbind(c(-0.5, 0.5))

fit_model <- function(data, cat) {
    tryCatch({
        if (cat == 'sameQ') {
            m <- glm(predictShared ~ experiment, data = data, family = binomial())
        } else {
            m <- glmer(predictShared ~ experiment + (1|pid), data = data, family = binomial(),
                       control = glmerControl(optimizer = 'bobyqa', optCtrl = list(maxfun = 100000)))
        }
        tibble(beta = coef(summary(m))[2, 1], pVal = coef(summary(m))[2, 4])
    }, error = function(e) tibble(beta = NA_real_, pVal = NA_real_))
}

results <- domain_df %>%
    mutate(matchedDomain = gsub('arbitrary', 'Lifestyle', matchedDomain)) %>%
    mutate(matchedDomain = tools::toTitleCase(matchedDomain)) %>%
    group_by(matchedDomain, category, stance) %>%
    nest() %>%
    mutate(res = map2(data, category, fit_model)) %>%
    select(-data) %>%
    unnest(res) %>%
    ungroup() %>%
    mutate(pVal_fdr = p.adjust(pVal, method = 'fdr'))

# Add significance stars
results <- results %>%
    mutate(sig = case_when(
        pVal_fdr < 0.001 ~ '***',
        pVal_fdr < 0.01 ~ '**',
        pVal_fdr < 0.05 ~ '*',
        TRUE ~ ''
    ))

# Format for display - pivot and reorder columns correctly
results_wide <- results %>%
    mutate(beta_sig = sprintf('%.2f%s', beta, sig)) %>%
    select(matchedDomain, category, stance, beta_sig) %>%
    pivot_wider(names_from = c(stance, category), values_from = beta_sig) %>%
    select(matchedDomain,
           opposing_sameQ, opposing_sameD, opposing_diffD,
           shared_sameQ, shared_sameD, shared_diffD)

knitr::kable(results_wide, col.names = c('Domain', 'Opp-Focal', 'Opp-Same', 'Opp-Diff', 'Shared-Focal', 'Shared-Same', 'Shared-Diff'))

# Summary stats
n_pos <- sum(results$beta > 0, na.rm = TRUE)
n_sig_pos <- sum(results$pVal_fdr < 0.05 & results$beta > 0, na.rm = TRUE)
n_sig_neg <- sum(results$pVal_fdr < 0.05 & results$beta < 0, na.rm = TRUE)

cat(sprintf('\nTotal models: %d\n', nrow(results)))
cat(sprintf('Positive coefficients: %d (%.1f%%)\n', n_pos, 100 * n_pos / nrow(results)))
cat(sprintf('Significant after FDR: %d positive, %d negative\n', n_sig_pos, n_sig_neg))
```

Over 95% of coefficients were positive, and every significant effect after FDR correction was in the positive direction (conversation increased commonality inference).

## LLM Prompt Template {#sec-llm-prompt}

The following prompt was used to elicit predictions from Gemini 2.5 Pro:

```
Predict how two people (Cat and Dog) would respond to survey questions
based on their conversation.

Likert scale:
1 = Definitely Not / Strongly Disagree
2 = Probably Not / Disagree
3 = Unsure / Neutral
4 = Probably Yes / Agree
5 = Definitely Yes / Strongly Agree

One question is marked [DISCUSSED] - they talked about this topic.
For other questions, infer from the conversation.

Base your predictions on:
- General population tendencies (as a starting point)
- Evidence from the conversation (update based on what they discuss)
- When conversation evidence is weak or absent, rely more on population priors

=== CONVERSATION ===
{conversation transcript}

=== QUESTIONS ===
{questions list with [DISCUSSED] marker}

=== TASK ===
Predict probability distributions for BOTH participants' responses
to ALL 35 questions.

Return JSON with this structure:
{
  "cat_predictions": {"0": {"1": p1, ..., "5": p5}, ...},
  "dog_predictions": {"0": {"1": p1, ..., "5": p5}, ...}
}
```

For the prior baseline (no conversation), we used a simplified prompt asking the model to predict responses for a randomly selected person based only on "general population tendencies" and "the nature of the question," with no conversation content.

## LLM Performance Details {#sec-llm-performance}

@tbl-llm-accuracy presents detailed LLM accuracy broken down by condition and question type.

```{python}
#| label: tbl-llm-accuracy
#| tbl-cap: "LLM prediction accuracy by condition and question type. Prior = population baseline with no individual information. Single observation = information-matched to Bayesian model. Full conversation = complete chat transcript."

# Re-import after R chunk (knitr breaks Python session)
import pandas as pd
from pathlib import Path
base_dir = Path.cwd().resolve()
while base_dir.name and not (base_dir / 'data').exists():
    base_dir = base_dir.parent

llm_dir = base_dir / 'data' / 'llm_results'

# Load single-observation (nochat) predictions
nochat_acc = pd.read_csv(llm_dir / 'nochat_accuracy.csv')
nochat_by_cat = {}
for cat in ['matched', 'same_domain', 'different_domain']:
    subset = nochat_acc[nochat_acc['category'] == cat]
    if len(subset) > 0:
        nochat_by_cat[cat] = subset['prob_correct'].mean() * 100

# Load full conversation (chat timecourse) predictions - use final time bin
chat_acc = pd.read_csv(llm_dir / 'chat_timecourse_accuracy.csv')
final_bin = chat_acc['time_bin'].max()
chat_final = chat_acc[chat_acc['time_bin'] == final_bin]
chat_by_cat = {}
for cat in ['matched', 'same_domain', 'different_domain']:
    subset = chat_final[chat_final['question_category'] == cat]
    if len(subset) > 0:
        chat_by_cat[cat] = subset['prob_correct'].mean() * 100

# Prior baseline: ~20% uniform (1/5 chance)
prior_acc = 20.0  # Approximate uniform baseline

# Create summary table
summary_data = {
    'Condition': ['Prior (no information)', 'Single observation', 'Full conversation'],
    'Focal Topic': [f'{prior_acc:.1f}%', f"{nochat_by_cat.get('matched', 0):.1f}%", f"{chat_by_cat.get('matched', 0):.1f}%"],
    'Same Domain': [f'{prior_acc:.1f}%', f"{nochat_by_cat.get('same_domain', 0):.1f}%", f"{chat_by_cat.get('same_domain', 0):.1f}%"],
    'Different Domain': [f'{prior_acc:.1f}%', f"{nochat_by_cat.get('different_domain', 0):.1f}%", f"{chat_by_cat.get('different_domain', 0):.1f}%"]
}

llm_table = pd.DataFrame(summary_data)
print(llm_table.to_markdown(index=False))

# Print computed values for verification
print(f"\nComputed from data files:")
print(f"  Single obs - Focal: {nochat_by_cat.get('matched', 0):.1f}%, Same: {nochat_by_cat.get('same_domain', 0):.1f}%, Diff: {nochat_by_cat.get('different_domain', 0):.1f}%")
print(f"  Full conv  - Focal: {chat_by_cat.get('matched', 0):.1f}%, Same: {chat_by_cat.get('same_domain', 0):.1f}%, Diff: {chat_by_cat.get('different_domain', 0):.1f}%")
```

The prior baseline showed no gradient (approximately 18% across all question types), confirming that population-level knowledge alone cannot explain structured predictions. With a single observation, the LLM showed modest improvement concentrated on the focal topic. With full conversational content, accuracy improved substantially across all question types, with the characteristic gradient (focal > same domain > different domain) that mirrors human commonality inferences.

## Survey Questions by Domain {#sec-stimuli}

@tbl-stimuli presents the complete set of 35 survey questions organized by domain.

```{python}
#| label: tbl-stimuli
#| tbl-cap: "Survey questions by domain."

# Uses base_dir from previous chunk
data_dir = base_dir / 'data'
questions_df = pd.read_csv(data_dir / 'questions.csv')
questions_df['domain'] = questions_df['domain'].replace('arbitrary', 'Lifestyle').str.capitalize()

# Group by domain
stimuli_by_domain = questions_df.groupby('domain')['text'].apply(lambda x: ' / '.join(x)).reset_index()
stimuli_by_domain.columns = ['Domain', 'Questions']

print(stimuli_by_domain.to_markdown(index=False))
```
